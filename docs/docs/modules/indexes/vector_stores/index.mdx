---
sidebar_label: "Vector Stores"
sidebar_position: 3
---

import CodeBlock from "@theme/CodeBlock";

# Getting Started: Vector Stores

:::info
[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/vectorstore)
:::

A vector store is a particular type of database optimized for storing documents and their [embeddings](../../models/embeddings/), and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.

```typescript
interface VectorStore {
  /**
   * Add more documents to an existing VectorStore
   */
  addDocuments(documents: Document[]): Promise<void>;

  /**
   * Search for the most similar documents to a query
   */
  similaritySearch(
    query: string,
    k?: number,
    filter?: object | undefined
  ): Promise<Document[]>;

  /**
   * Search for the most similar documents to a query,
   * and return their similarity score
   */
  similaritySearchWithScore(
    query: string,
    k = 4,
    filter: object | undefined = undefined
  ): Promise<[object, number][]>;

  /**
   * Turn a VectorStore into a Retriever
   */
  asRetriever(k?: number, similarityFilter?: SimilarityFilters): BaseRetriever;

  /**
   * Advanced: Add more documents to an existing VectorStore,
   * when you already have their embeddings
   */
  addVectors(vectors: number[][], documents: Document[]): Promise<void>;

  /**
   * Advanced: Search for the most similar documents to a query,
   * when you already have the embedding of the query
   */
  similaritySearchVectorWithScore(
    query: number[],
    k: number,
    filter?: object
  ): Promise<[Document, number][]>;
}
```

You can create a vector store from a list of [Documents](../../schema/document), or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.

```typescript
abstract class BaseVectorStore implements VectorStore {
  static fromTexts(
    texts: string[],
    metadatas: object[] | object,
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;

  static fromDocuments(
    docs: Document[],
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;
}
```

## Recursive Similarity Search

A problem some people may face is that when doing a similarity search, you have to supply a `k` value. This value is responsible for bringing N similar results back to you. But what if you don't know the `k` value? What if you want the system to return all the possible results?

In a real-world scenario, let's imagine a super long document created by a product manager which describes a product. In this document, we could have 10, 15, 20, 100 or more features described. How to know the correct `k` value so the system returns all the possible results to the question "What are all the features that product X has?".

To solve this problem, Langchain offers a feature called Recursive Similarity Search. With it, you can do a similarity search without having to rely solely on the `k` value. The system will return all the possible results to your question, based on the minimum similarity percentage you want.

It is possible to use the Recursive Similarity Search by using a vector store as retriever. Take a look at this example:

```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";

export const run = async () => {
  const vectorStore = await MemoryVectorStore.fromTexts(
    [
      "Buildings are made out of brick",
      "Buildings are made out of wood",
      "Buildings are made out of stone",
      "Cars are made out of metal",
      "Cars are made out of plastic",
    ],
    [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],
    new OpenAIEmbeddings()
  );

  const model = new ChatOpenAI({
    modelName: "gpt-3.5-turbo",
    temperature: 0,
  });

  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    // `1` is how many results in the first batch to fetch similar results. The `undefined` is the metadata filters
    vectorStore.asRetriever(1, undefined, {
      minSimilarityScore: 0.9, // Finds results with a similarity score of 90% or more
      dynamicK: true, // Uses a dynamic K value based on the minSimilarityScore. It'll keep increasing K until it can't find any more results
      maxK: 100, // The maximum K value to use. Use it based to your chunk size to make sure you don't run out of tokens
      kIncrement: 2, // How much to increase K by each time. It'll fetch N results, then N + kIncrement, then N + kIncrement * 2, etc.
    }),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question",
        outputKey: "text",
      }),
    }
  );
  const res = await chain.call({
    question: "Buildings are made out of what?",
  });

  console.log("response:", res); // {"text":"Buildings can be made out of various materials such as wood, brick, or stone.","sourceDocuments":[{"pageContent":"Buildings are made out of wood","metadata":{"id":2}},{"pageContent":"Buildings are made out of brick","metadata":{"id":1}},{"pageContent":"Buildings are made out of stone","metadata":{"id":3}}]}
};
```

## Which one to pick?

Here's a quick guide to help you pick the right vector store for your use case:

- If you're after something that can just run inside your Node.js application, in-memory, without any other servers to stand up, then go for [HNSWLib](./integrations/hnswlib), [Faiss](./integrations/faiss), or [LanceDB](./integrations/lancedb)
- If you're looking for something that can run in-memory in browser-like environments, then go for [MemoryVectorStore](./integrations/memory)
- If you come from Python and you were looking for something similar to FAISS, pick [HNSWLib](./integrations/hnswlib) or [Faiss](./integrations/faiss)
- If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for [Chroma](./integrations/chroma)
- If you're looking for an open-source production-ready vector database that you can run locally (in a docker container) or hosted in the cloud, then go for [Weaviate](./integrations/weaviate).
- If you're using Supabase already then look at the [Supabase](./integrations/supabase) vector store to use the same Postgres database for your embeddings too
- If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for [Pinecone](./integrations/pinecone)
- If you are already utilizing SingleStore, or if you find yourself in need of a distributed, high-performance database, you might want to consider the [SingleStore](./integrations/singlestore) vectore store.

## All Vector Stores

import DocCardList from "@theme/DocCardList";

<DocCardList />
