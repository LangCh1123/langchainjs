---
sidebar_label: "Similarity Score Thresholds"
sidebar_position: 3
---

# Getting Started with Recursive Similarity Search

A problem some people may face is that when doing a similarity search, you have to supply a `k` value. This value is responsible for bringing N similar results back to you. But what if you don't know the `k` value? What if you want the system to return all the possible results?

In a real-world scenario, let's imagine a super long document created by a product manager which describes a product. In this document, we could have 10, 15, 20, 100 or more features described. How to know the correct `k` value so the system returns all the possible results to the question "What are all the features that product X has?".

To solve this problem, Langchain offers a feature called Recursive Similarity Search. With it, you can do a similarity search without having to rely solely on the `k` value. The system will return all the possible results to your question, based on the minimum similarity percentage you want.

It is possible to use the Recursive Similarity Search by using a vector store as retriever. Take a look at this example:

```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";
import { SimilarityScoreThresholdVectorStoreRetriever } from "langchain/vectorstores";

export const run = async () => {
  const similarityScoreWrapper =
    new SimilarityScoreThresholdVectorStoreRetriever({
      minSimilarityScore: 0.9, // Finds results with a similarity score of 90% or more
      dynamicK: true, // Uses a dynamic K value based on the minSimilarityScore. It'll keep increasing K until it can't find any more results
      maxK: 100, // The maximum K value to use. Use it based to your chunk size to make sure you don't run out of tokens
      kIncrement: 2, // How much to increase K by each time. It'll fetch N results, then N + kIncrement, then N + kIncrement * 2, etc.
    });

  const vectorStore = await MemoryVectorStore.fromTexts(
    [
      "Buildings are made out of brick",
      "Buildings are made out of wood",
      "Buildings are made out of stone",
      "Cars are made out of metal",
      "Cars are made out of plastic",
    ],
    [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],
    new OpenAIEmbeddings()
  );

  const model = new ChatOpenAI({
    modelName: "gpt-3.5-turbo",
    temperature: 0,
  });

  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    similarityScoreWrapper.fromVectorStore(
      // `1` is how many results in the first batch to fetch similar results. Defaults to 4.
      vectorStore.asRetriever(1)
    ),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question",
        outputKey: "text",
      }),
    }
  );
  const res = await chain.call({
    question: "Buildings are made out of what?",
  });

  console.log("response:", res); // {"text":"Buildings can be made out of various materials such as wood, brick, or stone.","sourceDocuments":[{"pageContent":"Buildings are made out of wood","metadata":{"id":2}},{"pageContent":"Buildings are made out of brick","metadata":{"id":1}},{"pageContent":"Buildings are made out of stone","metadata":{"id":3}}]}
};
```
