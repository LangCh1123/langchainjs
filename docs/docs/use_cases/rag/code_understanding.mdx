# RAG over code

## Use case

Source code analysis is one of the most popular LLM applications (e.g., [GitHub Co-Pilot](https://github.com/features/copilot), [Code Interpreter](https://chat.openai.com/auth/login?next=%2F%3Fmodel%3Dgpt-4-code-interpreter), [Codium](https://www.codium.ai/), and [Codeium](https://codeium.com/about)) for use-cases such as:

- Q&A over the code base to understand how it works
- Using LLMs for suggesting refactors or improvements
- Using LLMs for documenting the code

![RAG over code](/img/rag_code_diagram.png)

## Overview

The pipeline for QA over code follows the [steps we do for document question answering](http://localhost:3000/docs/use_cases/question_answering/), with some differences:

In particular, we can employ a [splitting strategy](/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) that does a few things:

- Keeps each top-level function and class in the code is loaded into separate documents.
- Puts remaining into a separate document.
- Retains metadata about where each split comes from

## Quickstart

```bash
yarn add @supabase/supabase-js

# Set env var OPENAI_API_KEY or load from a .env file
```

### Loading

We'll upload all JavaScript/TypeScript files using the `DirectoryLoader` and `TextLoader` classes.

The following script iterates over the files in the LangChain repository and loads every `.ts` file (a.k.a. documents):

```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
```

```typescript
// Define the path to the repo to preform RAG on.
const REPO_PATH = "/tmp/test_repo";
```

We load the code by passing the directory path to `DirectoryLoader`, which will load all files with `.ts` extensions.
These files are then passed to a `TextLoader` which will return the contents of the file as a string.

```typescript
const loader = new DirectoryLoader(REPO_PATH, {
  ".ts": (path) => new TextLoader(path),
});
const docs = await loader.load();
```

Next, we can create a `RecursiveCharacterTextSplitter` to split our code.

We'll call the static `fromLanguage` method to create a splitter that knows how to split JavaScript/TypeScript code.

```typescript
const javascriptSplitter = RecursiveCharacterTextSplitter.fromLanguage("js", {
  chunkSize: 2000,
  chunkOverlap: 200,
});
const texts = await javascriptSplitter.splitDocuments(docs);

console.log("Loaded ", texts.length, " documents.");
```

```txt
Loaded 3324 documents.
```

### RetrievalQA

We need to store the documents in a way we can semantically search for their content.

The most common approach is to embed the contents of each document then store the embedding and document in a vector store.

When setting up the vector store retriever:

- We test max marginal relevance for retrieval
- And 5 documents returned

In this example we'll be using Supabase, however you can pick any vector store with MMR search you'd like from [our large list of integrations](/docs/modules/data_connection/vectorstores/integrations/).

```typescript
import { createClient } from "@supabase/supabase-js";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
```

```typescript
const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

const client = createClient(url, privateKey);
```

Once we've initialized our client we can pass it, along with some more options to the `.fromDocuments` method on `SupabaseVectorStore`.

For more instructions on how to set up Supabase, see the [Supabase docs](/docs/modules/data_connection/vectorstores/integrations/supabase).

```typescript
const vectorStore = await SupabaseVectorStore.fromDocuments(
  texts,
  new OpenAIEmbeddings(),
  {
    client,
    tableName: "documents",
    queryName: "match_documents",
  }
);
```

```typescript
const retriever = vectorStore.asRetriever({
  searchType: "mmr", // Use max marginal relevance search
  searchKwargs: { fetchK: 5 },
});
```

### Chat

We'll setup our model and memory system just as we'd do for any other chatbot application.

```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
```

```typescript
const model = new ChatOpenAI({ modelName: "gpt-4" });
```

We're going to use `BufferMemory` as our memory chain. All this will do is take in inputs/outputs from the LLM and store them in memory.

```typescript
import { BufferMemory } from "langchain/memory";
```

```typescript
const memory = new BufferMemory({
  returnMessages: true, // Return stored messages as instances of `BaseMessage`
  memoryKey: "chat_history", // This must match up with our prompt template input variable.
});
```

Now we can construct our main sequence of chains. We're going to be building `ConversationalRetrievalChain` using Expression Language.

```typescript
import { ChatPromptTemplate, MessagesPlaceholder } from "langchain/prompts";
import { RunnableBranch, RunnableSequence } from "langchain/schema/runnable";
import { formatDocumentsAsString } from "langchain/util/document";
import { BaseMessage } from "langchain/schema";
import { StringOutputParser } from "langchain/schema/output_parser";
```

First we construct the combine documents chain. This chain takes in the question, chat history (from memory) and context (relevant documents). It then passes all three into the prompt which answers the users question.

```typescript
const combineDocumentsPrompt = ChatPromptTemplate.fromMessages([
  [
    "ai",
    `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\n`,
  ],
  new MessagesPlaceholder("chat_history"),
  ["ai", "Helpful answer:"],
]);
```

```typescript
const combineDocumentsChain = RunnableSequence.from([
  {
    question: (i: { question: string }) => i.question,
    chat_history: async () => {
      const { chat_history } = await memory.loadMemoryVariables({});
      return chat_history;
    },
    context: async (i: { question: string }) => {
      const relevantDocs = await retriever.getRelevantDocuments(i.question);
      return formatDocumentsAsString(relevantDocs);
    },
  },
  combineDocumentsPrompt,
  model,
  new StringOutputParser(),
]);
```

Then, we define our question generator chain. This is used to regenerate the users question based on the chat history. Using LLMs to regenerate questions has been shown to lead to better outputs by LLMs since it can be updated to include past context.

```typescript
const question_generator_template = ChatPromptTemplate.fromMessages([
  [
    "ai",
    `Given the following conversation about a codebase and a follow up question, rephrase the follow up question to be a standalone question.`,
  ],
  new MessagesPlaceholder("chat_history"),
  [
    "ai",
    `Follow Up Input: {question}
Standalone question:`,
  ],
]);
```

```typescript
const questionGeneratorChain = RunnableSequence.from([
  {
    question: (i: { question: string }) => i.question,
    chat_history: async () => {
      const { chat_history } = await memory.loadMemoryVariables({});
      return chat_history;
    },
  },
  question_generator_template,
  model,
  new StringOutputParser(),
]);
```

In order to use the chain we defined above we create a new chain: `questionGeneratorAndCombineDocumentsChain`.

This chain is called whenever there is chat history so it can first rephrase the question, then pass it through to `combineDocumentsChain` for an answer.

```typescript
const questionGeneratorAndCombineDocumentsChain = RunnableSequence.from([
  {
    question: (i: { question: string }) => {
      return questionGeneratorChain.invoke(i);
    },
  },
  combineDocumentsChain,
]);
```

Finally, we'll patch it all together using a [`RunnableBranch`](/docs/expression_language/how_to/routing#using-a-runnablebranch).

The `RunnableBranch` allows us to pass in context, evaluate conditions, and invoke the first chain whose condition evaluated to true.

In our case we'll be checking if there is chat history. If no history is present, we do not need to rephrase a users question (since there's no chat history to rephrase with) and can directly call the `combineDocumentsChain`.

If there is chat history, we call the `questionGeneratorAndCombineDocumentsChain` which will rephrase the question and then pass it to `combineDocumentsChain`.

Since we're using a `RunnableBranch` we must also pass a fallback chain. This chain is called if none of the conditions evaluate to true.

```typescript
const conversationalQaChain = RunnableBranch.from([
  [
    (i: { chat_history: BaseMessage[]; question: string }) =>
      i.chat_history.length > 1,
    questionGeneratorAndCombineDocumentsChain,
  ],
  [
    (i: { chat_history: BaseMessage[]; question: string }) =>
      i.chat_history.length <= 1,
    combineDocumentsChain,
  ],
  combineDocumentsChain,
]);
```

The last step is to invoke our chain!

```typescript
const question = "How can I initialize a ReAct agent?";
const result = await conversationalQaChain.invoke({
  question,
  chat_history: (await memory.loadMemoryVariables({})).chat_history,
});
```

This is also where we'd save the LLM response to memory for future context.

```typescript
await memory.saveContext(
  {
    input: question,
  },
  {
    output: result,
  }
);
```

```typescript
console.log(result);
/**
You can initialize a ReAct agent using the "initializeAgentExecutorWithOptions" function provided by the "langchain/agents" package.

Here's an example code snippet:

\```javascript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });

const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
\```

That's it. Now you can use the "executor" instance to make requests through your agent.
 */
```

See the [LangSmith](https://smith.langchain.com) trace for this run [here](https://smith.langchain.com/public/d99bc61e-65a3-49d2-b797-0bd0138be0a2/r)

### Next steps

Since we're saving our inputs/outputs in memory we can ask followups to the LLM.

Keep in mind, we're not implementing an agent with tools so it must derive answers from the relevant documents in our store. Because of this it may return answers with hallucinated imports, classes or more. 

```typescript
const question2 =
  "How can I import and use the Wikipedia and File System tools from LangChain instead?";
const result2 = await conversationalQaChain.invoke({
  question: question2,
  chat_history: (await memory.loadMemoryVariables({})).chat_history,
});
```

```typescript
console.log(result2);
/**
You can import and utilize the Wikipedia and File System tools from LangChain in your codebase by following these steps,

1. Import the necessary tools:

\```javascript
import { ReadFileTool, WriteFileTool, NodeFileStore } from "langchain/tools";
import { WikipediaQueryRun } from "langchain/tools";
\```

2. Initialize the file store:

\```javascript
const store = new NodeFileStore();
\```

3. Initialize your tools:

\```javascript
const tools = [
  new ReadFileTool({ store }),
  new WriteFileTool({ store }),
];
\```

4. Initialize the Wikipedia query tool:

\```javascript
const wikipediaTool = new WikipediaQueryRun({
  topKResults: 3,
  maxDocContentLength: 4000,
});
\```

5. You can now use these tools for file read/write operations and to run Wikipedia queries. For example, to run a query on Wikipedia:

\```javascript
const result = await wikipediaTool.call("Query term");
console.log(result);
\```
 */
```

See the [LangSmith](https://smith.langchain.com) trace for this run [here](https://smith.langchain.com/public/21372f1a-edd5-4751-8788-a5da5f8a0ffb/r)

