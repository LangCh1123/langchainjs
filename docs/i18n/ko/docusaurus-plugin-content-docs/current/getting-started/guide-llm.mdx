---
sidebar_position: 2
---

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/llm/llm_streaming_stdout.ts";

# 빠른 시작, LLM 사용

이 튜토리얼은 LangChain을 사용하여 end-to-end 언어 모델 애플리케이션을 구축하는 방법에 대한 빠른 안내서를 제공합니다.

## 초기 세팅과 설치

시작하려면, [설치 가이드](./install)을 참고해 LangChain을 설치하세요.

## LLM 선택하기

LangChain을 사용하면 대개 하나 이상의 모델 제공자, 데이터 저장소, API 등과 통합이 필요합니다.

이 예제에서는 OpenAI의 API를 사용하므로 추가 설정이 필요하지 않습니다.

## 언어 모델 애플리케이션 구축하기

LangChain을 설치했다면, 이제 언어 모델 애플리케이션을 구축할 수 있습니다.

LangChain은 언어 모델 애플리케이션을 구축하는 데 사용할 수 있는 많은 모듈을 제공합니다. 모듈은 더 복잡한 애플리케이션을 만들기 위해 결합되거나, 간단한 애플리케이션에 대해 개별적으로 사용될 수 있습니다.

### LLMs: 언어모델로부터 예측 결과 얻기

LangChain의 가장 기본적인 구성 요소는 입력값에 대해 LLM을 호출하는 것입니다. 간단한 예제를 살펴보겠습니다. 이를 위해 우리는 회사가 제작하는 제품을 기반으로 회사 이름을 생성하는 서비스를 구축하고 있다고 가정해봅시다.

가장 먼저 LLM 래퍼(wrapper)를 가져와야 합니다.

```typescript
import { OpenAI } from "langchain/llms/openai";
```

그런 다음 OpenAI 키에 대한 환경 변수를 설정해야 합니다. 세 가지 옵션이 있습니다.

1. `.env` 파일에 값을 설정하고 [dotenv](https://github.com/motdotla/dotenv) 패키지를 사용하여 읽을 수 있습니다.

```bash
OPENAI_API_KEY="..."
```

2. 또는 shell에서 다음 명령을 사용하여 환경 변수를 설정할 수 있습니다.

```bash
export OPENAI_API_KEY=sk-....
```

3. 또는 래퍼를 초기화할 때 다른 인자와 함께 설정해줘도 됩니다. 이 예시에서는 출력이 더 무작위적이기를 원하므로 높은 `temperature` 값으로 초기화하겠습니다.

```typescript
const model = new OpenAI({ openAIApiKey: "sk-...", temperature: 0.9 });
```

래퍼를 초기화했다면, 이제 입력값을 넣어 호출해 볼 수 있습니다!

```typescript
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log(res);
```

```shell
{ res: '\n\nFantasy Sockery' }
```

### 프롬프트 템플릿: LLM 프롬프트 관리

LLM을 호출하는 것은 좋은 첫 단계이지만, 시작에 불과합니다. 일반적으로 애플리케이션에서 LLM을 사용할 때 사용자 입력을 LLM에 직접 전송하지 않습니다. 대신 사용자 입력을 가져와 프롬프트를 구성한 다음 LLM에 전송하게 될 것입니다.

예를 들어, 이전 예제에서는 컬러풀한 양말을 만드는 회사의 이름을 요청하는 하드코딩된 텍스트를 전달했습니다. 이제 이것을 회사가 무엇을 하는지 설명하는 사용자 입력을 받아 그 정보로 프롬프트를 만든 후 전달하도록 수정해보겠습니다.

LangChain을 사용하면 쉽게 할 수 있습니다!

먼저 프롬프트 템플릿을 정의합시다.

```typescript
import { PromptTemplate } from "langchain/prompts";

const template = "What is a good name for a company that makes {product}?";
const prompt = new PromptTemplate({
  template: template,
  inputVariables: ["product"],
});
```

이제 어떻게 작동하는지 살펴보겠습니다! `.format` 메서드를 호출하여 형식을 지정할 수 있습니다.

```typescript
const res = await prompt.format({ product: "colorful socks" });
console.log(res);
```

```shell
{ res: 'What is a good name for a company that makes colorful socks?' }
```

### 체인: 다단계 워크플로우에서 LLM과 프롬프트 결합하기

지금까지 PromptTemplate과 LLM 기본 요소를 각각 사용해봤습니다. 하지만 실제 애플리케이션은 이렇게 하나의 기본 요소가 아니라 여러 요소들의 조합으로 이뤄집니다.

LangChain에서 체인은 LLM과 같은 기본 요소나 다른 체인들로이 링크처럼 구성되어 있는 형태입니다.

가장 핵심적인 체인 타입은 LLMChain으로, 이는 PromptTemplate과 LLM으로 구성됩니다.

이전 예제를 확장하여, 사용자 입력을 받아 PromptTemplate으로 형식을 지정한 후 그 프롬프트를 LLM에 전달하는 LLMChain을 구성해보겠습니다.

```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

const model = new OpenAI({ temperature: 0.9 });
const template = "What is a good name for a company that makes {product}?";
const prompt = new PromptTemplate({
  template: template,
  inputVariables: ["product"],
});
```

사용자 입력을 받아 프롬프트로 형식을 지정한 다음 LLM으로 전송하는 매우 간단한 체인을 만들어 봅시다.

```typescript
import { LLMChain } from "langchain/chains";

const chain = new LLMChain({ llm: model, prompt: prompt });
```

이제 제품이 뭔지만 입력해주면 이 체인을 실행할 수 있습니다!

```typescript
const res = await chain.call({ product: "colorful socks" });
console.log(res);
```

```shell
{ res: { text: '\n\nColorfulCo Sockery.' } }
```

다 됐습니다! 우리는 첫 번째 체인으로 LLM 체인을 만들어봤습니다. LLM 체인은 간단한 편에 속하지만, 분명 더 복잡한 체인을 이해하는 데에 도움이 될 것입니다.

### 에이전트: 사용자 입력에 기반하여 동적으로 체인 실행하기

지금까지 살펴본 체인은 우리가 미리 결정해 놓은 순서로 실행됩니다.

하지만, 에이전트를 사용하면 더 이상 그렇게 할 필요가 없습니다. 에이전트는 어떤 작업을 수행하고 어떤 순서로 할지를 결정하기 위해 LLM을 사용합니다. 이 에이전트가 취할 수 있는 액션은 도구를 사용하고 그 결과를 관찰하거나, 혹은 사용자에게 결괏값을 돌려주는 것입니다.

올바르게만 사용된다면 에이전트는 매우 강력할 수 있습니다. 이 튜토리얼에서는 가장 간단하지만 높은 수준의 API를 통해 에이전트를 쉽게 사용하는 방법을 보여줍니다.

에이전트를 로드하려면 다음 개념을 이해해야 합니다.

- 도구(Tool): 특정 업무를 수행하는 함수입니다. Google 검색, 데이터베이스 조회, 코드 REPL, 다른 체인 등이 있을 수 있습니다. 도구의 인터페이스는 현재 string을 입력으로 받아 string을 출력하는 함수입니다.
- LLM: 에이전트를 구동하는 언어 모델입니다.
- 에이전트(Agent): 사용할 에이전트입니다. 이는 특정 에이전트 클래스를 참조하는 string이어야 합니다. 이 튜토리얼은 가장 간단하지만 높은 수준의 API에 중점을 두고 있으므로 기본적인 에이전트만 다룹니다.

이 예제를 따라하기 위해서는 `.env` 파일에 SerpAPI 환경 변수를 설정해야 합니다.

```bash
SERPAPI_API_KEY="..."
```

`serpapi` 패키지를 설치합니다. (Google Search API)

```bash npm2yarn
npm install -S serpapi
```

이제 본격적으로 시작해보겠습니다!

```typescript
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
console.log("Loaded agent.");

const input =
  "Who is Olivia Wilde's boyfriend?" +
  " What is his current age raised to the 0.23 power?";
console.log(`Executing with input "${input}"...`);

const result = await executor.call({ input });

console.log(`Got output ${result.output}`);
```

```shell
langchain-examples:start: Executing with input "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"...
langchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.
```

### 메모리: 체인과 에이전트에 상태 추가하기

지금까지 살펴본 모든 체인과 에이전트에는 상태가 없습니다. 그러나 종종 체인이나 에이전트에 이전 상호작용에 대한 정보를 기억하는 "메모리" 개념이 필요할 수 있습니다. 가장 명확하고 간단한 예로, 채팅봇을 설계할 때 이전 메시지를 기억하여 더 나은 대화를 나눌 수 있도록 그 맥락을 사용하고 싶을 수 있습니다. 이것은 "단기 메모리"의 한 형태입니다. 더 복잡하게는 체인/에이전트가 시간이 지남에 따라 중요한 정보를 기억하는 것을 상상해 볼 수도 있습니다. 이것은 "장기 메모리"의 한 형태가 될 것입니다.

LangChain은 이를 위해 특별히 만들어진 여러 체인을 제공합니다. 이 섹션에서는 그러한 체인 중 하나인 `ConversationChain`을 사용하는 방법에 대해 알아봅니다.

기본적으로 `ConversationChain`은 이전 입/출력을 모두 기억하고 맥락에 추가하는 간단한 유형의 메모리를 가지고 있습니다. 이 체인을 사용하는 방법을 살펴보겠습니다.

```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log(res1);
```

```shell
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```

```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log(res2);
```

```shell
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```

## 스트리밍

스트리밍 API를 사용하여 각 단어가 생성될 때마다 단어를 실시간 스트림으로 받을 수도 있습니다. 예를 들어 채팅봇에서 사용자에게 생성되는 내용을 생성되는 즉시 보여주고자 할 때 유용하게 사용할 수 있습니다. 참고로, 이 문서를 작성하는 시점에는 스트리밍 옵션이 활성화되면 OpenAI에서 `tokenUsage`를 지원하지 않습니다.

<CodeBlock language="typescript">{Example}</CodeBlock>
