{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "hide_table_of_contents: true\n",
    "sidebar_position: 0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool calling\n",
    "\n",
    ":::tip Compatibility\n",
    "Tool calling is only available with [supported models](/docs/integrations/chat/).\n",
    ":::\n",
    "\n",
    "[Tool calling](/docs/modules/model_io/chat/function_calling) allows a model to respond to a given prompt by generating output that matches a user-defined schema.\n",
    "By supplying the model with a schema that matches up with a [LangChain tool's](/docs/modules/agents/tools/) signature, along with a name and description of what the tool does, we can get the model\n",
    "to reliably generate valid input. We can take advantage of this structured output, combined with the fact that you can bind multiple tools to a [tool calling chat model](/docs/integrations/chat/) and\n",
    "allow the model to choose which one to call, to create an agent that repeatedly calls tools and receives results until a query is resolved.\n",
    "\n",
    "This is a more generalized version of the [OpenAI tools agent](/docs/modules/agents/agent_types/openai_tools_agent/), which was designed for OpenAI's specific style of\n",
    "tool calling. It uses LangChain's [ToolCall](https://api.js.langchain.com/types/langchain_core_messages_tool.ToolCall.html) representation to support a wider range of\n",
    "provider implementations, such as [Anthropic](/docs/integrations/chat/anthropic/), [Google Gemini](/docs/integrations/chat/google_vertex_ai), and [Mistral](/docs/integrations/chat/mistral/)\n",
    "in addition to [OpenAI](/docs/integrations/chat/openai/).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Most models that support tool calling can be used in this agent. See [this list](/docs/integrations/chat/) for the most up-to-date information.\n",
    "\n",
    "This demo also uses [Tavily](https://app.tavily.com), but you can also swap in another [built in tool](/docs/integrations/platforms).\n",
    "You'll need to sign up for an API key and set it as `process.env.TAVILY_API_KEY`.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" additionalDependencies=\"@langchain/community\" />\n",
    "```\n",
    "\n",
    "## Initialize Tools\n",
    "\n",
    "We will first create a tool that can search the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "// Define the tools the agent will have access to.\n",
    "const tools = [new TavilySearchResults({ maxResults: 1 })];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "Next, let's initialize our tool calling agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AgentExecutor, createToolCallingAgent } from \"langchain/agents\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "// Prompt template must have \"input\" and \"agent_scratchpad input variables\"\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are a helpful assistant\"],\n",
    "  [\"placeholder\", \"{chat_history}\"],\n",
    "  [\"human\", \"{input}\"],\n",
    "  [\"placeholder\", \"{agent_scratchpad}\"],\n",
    "]);\n",
    "\n",
    "const agent = await createToolCallingAgent({\n",
    "  llm,\n",
    "  tools,\n",
    "  prompt,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Agent\n",
    "\n",
    "Now, let's initialize the executor that will run our agent and invoke it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const agentExecutor = new AgentExecutor({\n",
    "  agent,\n",
    "  tools,\n",
    "});\n",
    "\n",
    "const result = await agentExecutor.invoke({\n",
    "  input: \"what is LangChain?\",\n",
    "});\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::tip\n",
    "[LangSmith trace](https://smith.langchain.com/public/5c125a7e-0df5-41ec-96bf-3c13dc3a53f8/r)\n",
    ":::\n",
    "\n",
    "## Using with chat history\n",
    "\n",
    "This type of agent can optionally take chat messages representing previous conversation turns. For more details, see [this section of the agent quickstart](/docs/modules/agents/quick_start#adding-in-memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AIMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const result2 = await agentExecutor.invoke({\n",
    "  input: \"what's my name?\",\n",
    "  chat_history: [\n",
    "    new HumanMessage(\"hi! my name is cob\"),\n",
    "    new AIMessage(\"Hello Cob! How can I assist you today?\"),\n",
    "  ],\n",
    "});\n",
    "\n",
    "console.log(result2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
