{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Tools\n",
    "\n",
    "These output parsers extract tool calls from OpenAI’s function calling API responses. This means they are only usable with models that support function calling, and specifically the latest `tools` and `tool_choice` parameters. We recommend familiarizing yourself with [function calling](/docs/modules/model_io/chat/function_calling) before reading this guide.\n",
    "\n",
    "There are a few different variants of output parsers:\n",
    "\n",
    "- [`JsonOutputToolsParser`](https://api.js.langchain.com/classes/langchain_output_parsers.JsonOutputToolsParser.html): Returns the arguments of the function call as JSON\n",
    "- [`JsonOutputKeyToolsParser`](https://api.js.langchain.com/classes/langchain_output_parsers.JsonOutputKeyToolsParser.html): Returns the value of specific key in the function call as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from '@langchain/core/prompts';\n",
    "import { ChatOpenAI } from '@langchain/openai';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const properties = {\n",
    "  setup: {\n",
    "    type: \"string\",\n",
    "    description: \"The setup for the joke\"\n",
    "  },\n",
    "  punchline: {\n",
    "    type: \"string\",\n",
    "    description: \"The joke's punchline\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const tool = {\n",
    "  type: \"function\" as const,\n",
    "  function: {\n",
    "    name: \"joke\",\n",
    "    description: \"Joke to tell user.\",\n",
    "    parameters: {\n",
    "      $schema: \"http://json-schema.org/draft-07/schema#\",\n",
    "      title: \"Joke\",\n",
    "      type: \"object\",\n",
    "      properties,\n",
    "      required: [\"setup\", \"punchline\"]\n",
    "    },\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const llm = new ChatOpenAI();\n",
    "\n",
    "// Use `.bind` to attach the tool to the model\n",
    "const llmWithTools = llm.bind({\n",
    "  tools: [tool],\n",
    "  // Optionally, we can pass the tool to the `tool_choice` parameter to\n",
    "  // force the model to call the tool.\n",
    "  tool_choice: tool,\n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are the funniest comedian, tell the user a joke about their topic.\"],\n",
    "  [\"human\", \"Topic: {topic}\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use LCEL to pipe our prompt and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(llmWithTools);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"{\\n\"\u001b[39m +\n",
       "  \u001b[32m'  \"lc\": 1,\\n'\u001b[39m +\n",
       "  \u001b[32m'  \"type\": \"constructor\",\\n'\u001b[39m +\n",
       "  \u001b[32m'  \"id\": [\\n'\u001b[39m +\n",
       "  \u001b[32m'    \"langchain_core\",\\n'\u001b[39m +\n",
       "  \u001b[32m'    \"messages\",\\n'\u001b[39m +\n",
       "  \u001b[32m'    \"AIMessage'\u001b[39m... 454 more characters"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON.stringify(await chain.invoke({ topic: \"Large Language Models\" }), null, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Inspect the [LangSmith trace](https://smith.langchain.com/public/2413c983-f803-4bea-8ea1-adadf6a0af32/r) from the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JsonOutputToolsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { JsonOutputToolsParser } from \"langchain/output_parsers\";\n",
    "\n",
    "const outputParser = new JsonOutputToolsParser();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(llmWithTools).pipe(outputParser);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"[\\n\"\u001b[39m +\n",
       "  \u001b[32m\"  {\\n\"\u001b[39m +\n",
       "  \u001b[32m'    \"type\": \"joke\",\\n'\u001b[39m +\n",
       "  \u001b[32m'    \"args\": {\\n'\u001b[39m +\n",
       "  \u001b[32m'      \"setup\": \"Why did the large language model start a ban'\u001b[39m... 84 more characters"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON.stringify(await chain.invoke({ topic: \"Large Language Models\" }), null, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Inspect the [LangSmith trace](https://smith.langchain.com/public/1981c0f9-6059-4f4b-a29e-579aa43c571b/r) with the `JsonOutputToolsParser` output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JsonOutputKeyToolsParser\n",
    "\n",
    "This merely extracts a single key from the returned response. This is useful for when you are passing in a single tool and just want it’s arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { JsonOutputKeyToolsParser } from \"langchain/output_parsers\";\n",
    "\n",
    "const outputParser = new JsonOutputKeyToolsParser({ keyName: \"joke\" });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(llmWithTools).pipe(outputParser);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"[\\n\"\u001b[39m +\n",
       "  \u001b[32m\"  {\\n\"\u001b[39m +\n",
       "  \u001b[32m'    \"setup\": \"Why did the large language model go on a diet?\",\\n'\u001b[39m +\n",
       "  \u001b[32m`    \"punchline\": \"It couldn't s`\u001b[39m... 38 more characters"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON.stringify(await chain.invoke({ topic: \"Large Language Models\" }), null, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Inspect the [LangSmith trace](https://smith.langchain.com/public/698f4395-37dd-4c47-9dbe-31b0f9414987/r) with the `JsonOutputKeyToolsParser` output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LLMs have support for calling multiple tools in a single response. Because of this, the result of invoking `JsonOutputKeyToolsParser` is always an array. If you would only like a single result to be returned, you can specify `returnSingle` in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "const outputParserAll = new JsonOutputKeyToolsParser({\n",
    "  keyName: \"joke\",\n",
    "});\n",
    "const outputParserSingle = new JsonOutputKeyToolsParser({\n",
    "  keyName: \"joke\",\n",
    "  returnSingle: true,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(llmWithTools);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "const response = await chain.invoke({ topic: \"Large Language Models\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without `returnSingle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m'[{\"setup\":\"Why did the large language model bring a flashlight?\",\"punchline\":\"Because it heard it sh'\u001b[39m... 49 more characters"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON.stringify(await outputParserAll.invoke(response));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the [LangSmith trace](https://smith.langchain.com/public/4733255d-d413-42a2-9094-d4f59b1c552c/r) from this output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `returnSingle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m'{\"setup\":\"Why did the large language model bring a flashlight?\",\"punchline\":\"Because it heard it sho'\u001b[39m... 47 more characters"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON.stringify(await outputParserSingle.invoke(response));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the [LangSmith trace](https://smith.langchain.com/public/6f6225c0-ad4c-40c7-81c1-4f26d0e4b085/r) from this output parser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
