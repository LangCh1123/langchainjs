{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "\n",
    "How can we get a model to cite which parts of the source documents it referenced in its response?\n",
    "\n",
    "To explore some techniques for extracting citations, let's first create a simple RAG chain. To start we'll just retrieve from the web using the [ExaRetriever](https://js.langchain.com/docs/integrations/retrievers/exa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Dependencies\n",
    "\n",
    "We’ll use an OpenAI chat model and embeddings and a Memory vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/modules/model_io/chat) or [LLM](/docs/modules/model_io/llms), [Embeddings](https://js.langchain.com/docs/modules/data_connection/text_embedding/), and [VectorStore](https://js.langchain.com/docs/modules/data_connection/vectorstores/) or [Retriever](/docs/modules/data_connection/retrievers/).\n",
    "\n",
    "We’ll use the following packages:\n",
    "\n",
    "```bash\n",
    "npm install --save langchain @langchain/exa @langchain/openai cheerio\n",
    "```\n",
    "\n",
    "We need to set environment variables for Exa & OpenAI:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=YOUR_KEY\n",
    "export EXASEARCH_API_KEY=YOUR_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
    "\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_API_KEY=YOUR_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exa from \"exa-js\";\n",
    "import { ExaRetriever } from \"@langchain/exa\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const exaClient = new Exa();\n",
    "const llm = new ChatOpenAI({\n",
    "  modelName: \"gpt-3.5-turbo\",\n",
    "  temperature: 0,\n",
    "});\n",
    "const search = new ExaRetriever({\n",
    "  client: exaClient,\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\\n\\nHere are the Wikipedia articles:{context}\"],\n",
    "  [\"human\", \"{question}\"],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a model, retriever and prompt, let's chain them all together. We'll need to add some logic for formatting our retrieved `Document`s to a string that can be passed to our prompt. We'll make it so our chain returns both the answer and the retrieved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Document } from \"@langchain/core/documents\";\n",
    "\n",
    "/**\n",
    " * Get the page content of the documents, and format into a string.\n",
    " */\n",
    "const formatDocs = async (docs: Array<Document>): Promise<string> => {\n",
    "  const results = await exaClient.getContents(docs.map((doc) => doc.metadata.id));\n",
    "  const formatted = docs.map((doc, idx) => `Article title: ${doc.metadata.title}\\nArticle Snippet: ${results[idx].}`);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
