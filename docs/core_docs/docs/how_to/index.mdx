---
sidebar_position: 0
sidebar_class_name: hidden
---

# How-to Guides

Here you'll find short answers to “How do I….?” types of questions.
These how-to guides don't cover topics in depth - you'll find that material in the [Tutorials](/v0.2/docs/tutorials) and the [API Reference](https://api.js.langchain.com).
However, these guides will help you quickly accomplish common tasks.

## Core Functionality

This covers functionality that is core to using LangChain

- [How to return structured data from an LLM](/v0.2/docs/how_to/structured_output/)
- [How to use a chat model to call tools](/v0.2/docs/how_to/tool_calling/)
- [How to stream](/v0.2/docs/how_to/streaming)
- [How to debug your LLM apps](/v0.2/docs/how_to/debugging/)

## LangChain Expression Language (LCEL)

LangChain Expression Language a way to create arbitrary custom chains.

- [How to combine multiple runnables into a chain](/v0.2/docs/how_to/sequence)
- [How to invoke runnables in parallel](/v0.2/docs/how_to/parallel/)
- [How to attach runtime arguments to a runnable](/v0.2/docs/how_to/binding/)
- [How to run custom functions](/v0.2/docs/how_to/functions)
- [How to pass through arguments from one step to the next](/v0.2/docs/how_to/passthrough)
- [How to add values to a chain's state](/v0.2/docs/how_to/assign)
- [How to configure a chain at runtime](/v0.2/docs/how_to/configure)
- [How to add message history](/v0.2/docs/how_to/message_history)
- [How to route execution within a chain](/v0.2/docs/how_to/routing)
- [How to inspect your runnables](/v0.2/docs/how_to/inspect)
- [How to add fallbacks](/v0.2/docs/how_to/fallbacks)

## Components

These are the core building blocks you can use when building applications.

### Prompt Templates

Prompt Templates are responsible for formatting user input into a format that can be passed to a language model.

- [How to use few shot examples](/v0.2/docs/how_to/few_shot_examples)
- [How to use few shot examples in chat models](/v0.2/docs/how_to/few_shot_examples_chat/)
- [How to partially format prompt templates](/v0.2/docs/how_to/prompts_partial)
- [How to compose prompts together](/v0.2/docs/how_to/prompts_composition)

### Example Selectors

Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to use example selectors](/v0.2/docs/how_to/example_selectors)
- [How to select examples by length](/v0.2/docs/how_to/example_selectors_length_based)
- [How to select examples by semantic similarity](/v0.2/docs/how_to/example_selectors_similarity)
- [How to select examples by semantic ngram overlap](/v0.2/docs/how_to/example_selectors_ngram)
- [How to select examples by maximal marginal relevance](/v0.2/docs/how_to/example_selectors_mmr)

### Chat Models

Chat Models are newer forms of language models that take messages in and output a message.

- [How to do function/tool calling](/v0.2/docs/how_to/tool_calling)
- [How to get models to return structured output](/v0.2/docs/how_to/structured_output)
- [How to cache model responses](/v0.2/docs/how_to/chat_model_caching)
- [How to get log probabilities from model calls](/v0.2/docs/how_to/logprobs)
- [How to create a custom chat model class](/v0.2/docs/how_to/custom_chat_model)
- [How to stream a response back](/v0.2/docs/how_to/chat_streaming)
- [How to track token usage](/v0.2/docs/how_to/chat_token_usage_tracking)

### LLMs

What LangChain calls LLMs are older forms of language models that take a string in and output a string.

- [How to cache model responses](/v0.2/docs/how_to/llm_caching)
- [How to create a custom LLM class](/v0.2/docs/how_to/custom_llm)
- [How to stream a response back](/v0.2/docs/how_to/streaming_llm)
- [How to track token usage](/v0.2/docs/how_to/llm_token_usage_tracking)

### Output Parsers

Output Parsers are responsible for taking the output of an LLM and parsing into more structured format.

- [How to use output parsers to parse an LLM response into structured format](/v0.2/docs/how_to/output_parser_structured)
- [How to parse JSON output](/v0.2/docs/how_to/output_parser_json)
- [How to parse XML output](/v0.2/docs/how_to/output_parser_xml)
- [How to parse YAML output](/v0.2/docs/how_to/output_parser_yaml)
- [How to retry when output parsing errors occur](/v0.2/docs/how_to/output_parser_retry)
- [How to try to fix errors in output parsing](/v0.2/docs/how_to/output_parser_fixing)
- [How to write a custom output parser class](/v0.2/docs/how_to/output_parser_custom)

### Document Loaders

Document Loaders are responsible for loading documents from a variety of sources.

- [How to load CSV data](/v0.2/docs/how_to/document_loader_csv)
- [How to load data from a directory](/v0.2/docs/how_to/document_loader_directory)
- [How to load HTML data](/v0.2/docs/how_to/document_loader_html)
- [How to load JSON data](/v0.2/docs/how_to/document_loader_json)
- [How to load Markdown data](/v0.2/docs/how_to/document_loader_markdown)
- [How to load Microsoft Office data](/v0.2/docs/how_to/document_loader_office_file)
- [How to load PDF files](/v0.2/docs/how_to/document_loader_pdf)
- [How to write a custom document loader](/v0.2/docs/how_to/document_loader_custom)

### Text Splitters

Text Splitters take a document and split into chunks that can be used for retrieval.

- [How to recursively split text](/v0.2/docs/how_to/recursive_text_splitter)
- [How to split by HTML headers](/v0.2/docs/how_to/HTML_header_metadata_splitter)
- [How to split by HTML sections](/v0.2/docs/how_to/HTML_section_aware_splitter)
- [How to split by character](/v0.2/docs/how_to/character_text_splitter)
- [How to split code](/v0.2/docs/how_to/code_splitter)
- [How to split Markdown by headers](/v0.2/docs/how_to/markdown_header_metadata_splitter)
- [How to recursively split JSON](/v0.2/docs/how_to/recursive_json_splitter)
- [How to split text into semantic chunks](/v0.2/docs/how_to/semantic-chunker)
- [How to split by tokens](/v0.2/docs/how_to/split_by_token)

### Embedding Models

Embedding Models take a piece of text and create a numerical representation of it.

- [How to embed text data](/v0.2/docs/how_to/embed_text)
- [How to cache embedding results](/v0.2/docs/how_to/caching_embeddings)

### Vector Stores

Vector Stores are databases that can efficiently store and retrieve embeddings.

- [How to use a vector store to retrieve data](/v0.2/docs/how_to/vectorstores)

### Retrievers

Retrievers are responsible for taking a query and returning relevant documents.

- [How use a vector store to retrieve data](/v0.2/docs/how_to/vectorstore_retriever)
- [How to generate multiple queries to retrieve data for](/v0.2/docs/how_to/MultiQueryRetriever)
- [How to use contextual compression to compress the data retrieved](/v0.2/docs/how_to/contextual_compression)
- [How to write a custom retriever class](/v0.2/docs/how_to/custom_retriever)
- [How to combine the results from multiple retrievers](/v0.2/docs/how_to/ensemble_retriever)
- [How to reorder retrieved results to put most relevant documents not in the middle](/v0.2/docs/how_to/long_context_reorder)
- [How to generate multiple embeddings per document](/v0.2/docs/how_to/multi_vector)
- [How to retrieve the whole document for a chunk](/v0.2/docs/how_to/parent_document_retriever)
- [How to generate metadata filters](/v0.2/docs/how_to/self_query)
- [How to create a time-weighted retriever](/v0.2/docs/how_to/time_weighted_vectorstore)

### Indexing

Indexing is the process of keeping your vectorstore in-sync with the underlying data source.

- [How to reindex data to keep your vectorstore in-sync with the underlying data source](/v0.2/docs/how_to/indexing)

### Tools

LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call).

- [How to use LangChain tools](/v0.2/docs/how_to/tools)
- [How to use a chat model to call tools](/v0.2/docs/how_to/tool_calling/)
- [How to use LangChain toolkits](/v0.2/docs/how_to/toolkits)
- [How to define a custom tool](/v0.2/docs/how_to/custom_tools)
- [How to convert LangChain tools to OpenAI functions](/v0.2/docs/how_to/tools_as_openai_functions)
- [How to use tools without function calling](/v0.2/docs/how_to/tools_prompting)
- [How to let the LLM choose between multiple tools](/v0.2/docs/how_to/tools_multiple)
- [How to add a human in the loop to tool usage](/v0.2/docs/how_to/tools_human)
- [How to do parallel tool use](/v0.2/docs/how_to/tools_parallel)
- [How to handle errors when calling tools](/v0.2/docs/how_to/tools_error)

### Agents

:::note

For in depth how-to guides for agents, please check out [LangGraph](https://github.com/langchain-ai/langgraph) documentation.

:::

- [How to use legacy LangChain Agents (AgentExecutor)](/v0.2/docs/how_to/agent_executor)
- [How to migrate from legacy LangChain agents to LangGraph](/v0.2/docs/how_to/migrate_agent)

### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to create a custom chat model class](/v0.2/docs/how_to/custom_chat_model)
- [How to create a custom LLM class](/v0.2/docs/how_to/custom_llm)
- [How to write a custom retriever class](/v0.2/docs/how_to/custom_retriever)
- [How to write a custom document loader](/v0.2/docs/how_to/document_loader_custom)
- [How to write a custom output parser class](/v0.2/docs/how_to/output_parser_custom)

- [How to define a custom tool](/v0.2/docs/how_to/custom_tools)

## Use Cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.

- [How to add chat history](/v0.2/docs/how_to/qa_chat_history_how_to/)
- [How to stream](/v0.2/docs/how_to/qa_streaming/)
- [How to return sources](/v0.2/docs/how_to/qa_sources/)
- [How to return citations](/v0.2/docs/how_to/qa_citations/)
- [How to do per-user retrieval](/v0.2/docs/how_to/qa_per_user/)

### Extraction

Extraction is when you use LLMs to extract structured information from unstructured text.

- [How to use reference examples](/v0.2/docs/how_to/extraction_examples/)
- [How to handle long text](/v0.2/docs/how_to/extraction_long_text/)
- [How to do extraction without using function calling](/v0.2/docs/how_to/extraction_parse)

### Chatbots

Chatbots involve using an LLM to have a conversation.

- [How to manage memory](/v0.2/docs/how_to/chatbots_memory)
- [How to do retrieval](/v0.2/docs/how_to/chatbots_retrieval)
- [How to use tools](/v0.2/docs/how_to/chatbots_tools)

### Query Analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.

- [How to add examples to the prompt](/v0.2/docs/how_to/query_few_shot)
- [How to handle cases where no queries are generated](/v0.2/docs/how_to/query_no_queries)
- [How to handle multiple queries](/v0.2/docs/how_to/query_multiple_queries)
- [How to handle multiple retrievers](/v0.2/docs/how_to/query_multiple_retrievers)
- [How to construct filters](/v0.2/docs/how_to/query_constructing_filters)
- [How to deal with high cardinality categorical variables](/v0.2/docs/how_to/query_high_cardinality)

### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.

- [How to use prompting to improve results](/v0.2/docs/how_to/sql_prompting)
- [How to do query validation](/v0.2/docs/how_to/sql_query_checking)
- [How to deal with large databases](/v0.2/docs/how_to/sql_large_db)
- [How to deal with CSV files](/v0.2/docs/how_to/sql_csv)

### Q&A over Graph Databases

You can use an LLM to do question answering over graph databases.

- [How to map values to a database](/v0.2/docs/how_to/graph_mapping)
- [How to add a semantic layer over the database](/v0.2/docs/how_to/graph_semantic)
- [How to improve results with prompting](/v0.2/docs/how_to/graph_prompting)
- [How to construct knowledge graphs](/v0.2/docs/how_to/graph_constructing)
