{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e9437c8a-d8b7-4bf6-8ff4-54068a5a266c",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1.5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df7646-b1e1-4014-a841-6dae9b3c50d9",
   "metadata": {},
   "source": [
    "# How to stream chat model responses\n",
    "\n",
    "All [chat models](https://v02.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html) implement the [Runnable interface](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html), which comes with a **default** implementations of standard runnable methods (i.e. `invoke`, `batch`, `stream`, `streamEvents`).\n",
    "\n",
    "The **default** streaming implementation provides an `AsyncGenerator` that yields a single value: the final output from the underlying chat model provider.\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "The **default** implementation does **not** provide support for token-by-token streaming, but it ensures that the the model can be swapped in for any other model as it supports the same standard interface.\n",
    "\n",
    ":::\n",
    "\n",
    "The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.\n",
    "\n",
    "See which [integrations support token-by-token streaming here](/docs/integrations/chat/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76660e-7691-48b7-a2b4-2ccdff7875c3",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Below, we use a `---` to help visualize the delimiter between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c144d",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975c4f32-21f6-4a71-9091-f87b56347c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Sw\n",
      "---\n",
      "imming\n",
      "---\n",
      " in\n",
      "---\n",
      " a\n",
      "---\n",
      " world\n",
      "---\n",
      " of\n",
      "---\n",
      " silver\n",
      "---\n",
      " beams\n",
      "---\n",
      ",\n",
      "\n",
      "---\n",
      "Gold\n",
      "---\n",
      "fish\n",
      "---\n",
      " on\n",
      "---\n",
      " the\n",
      "---\n",
      " moon\n",
      "---\n",
      ",\n",
      "---\n",
      " living\n",
      "---\n",
      " their\n",
      "---\n",
      " dreams\n",
      "---\n",
      ".\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "const stream = await model.stream(\"Write me a 1 verse song about goldfish on the moon\")\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "    console.log(`${chunk.content}\n",
    "---`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e1309-3b6e-42fb-820a-2e4e3e6bc074",
   "metadata": {},
   "source": [
    "## Stream events\n",
    "\n",
    "Chat models also support the standard [streamEvents()](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html#streamEvents) method.\n",
    "\n",
    "This method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., a chain composed of a prompt, chat model and parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bd1dfd-8ae2-49d6-b526-97180c81b5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  event: 'on_chat_model_start',\n",
      "  data: { input: 'Write me a 1 verse song about goldfish on the moon' },\n",
      "  name: 'ChatOpenAI',\n",
      "  tags: [],\n",
      "  run_id: 'c9966059-70eb-4f24-9de3-2cf04320c8f6',\n",
      "  metadata: {\n",
      "    ls_provider: 'openai',\n",
      "    ls_model_name: 'gpt-3.5-turbo',\n",
      "    ls_model_type: 'chat',\n",
      "    ls_temperature: 1,\n",
      "    ls_max_tokens: undefined,\n",
      "    ls_stop: undefined\n",
      "  }\n",
      "}\n",
      "{\n",
      "  event: 'on_chat_model_stream',\n",
      "  data: {\n",
      "    chunk: AIMessageChunk {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: '',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: [Object],\n",
      "      id: 'chatcmpl-9lOQhe44ip2q0DHfr0eYU9TF4mHtu',\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: [],\n",
      "      tool_call_chunks: [],\n",
      "      usage_metadata: undefined\n",
      "    }\n",
      "  },\n",
      "  run_id: 'c9966059-70eb-4f24-9de3-2cf04320c8f6',\n",
      "  name: 'ChatOpenAI',\n",
      "  tags: [],\n",
      "  metadata: {\n",
      "    ls_provider: 'openai',\n",
      "    ls_model_name: 'gpt-3.5-turbo',\n",
      "    ls_model_type: 'chat',\n",
      "    ls_temperature: 1,\n",
      "    ls_max_tokens: undefined,\n",
      "    ls_stop: undefined\n",
      "  }\n",
      "}\n",
      "{\n",
      "  event: 'on_chat_model_stream',\n",
      "  run_id: 'c9966059-70eb-4f24-9de3-2cf04320c8f6',\n",
      "  name: 'ChatOpenAI',\n",
      "  tags: [],\n",
      "  metadata: {\n",
      "    ls_provider: 'openai',\n",
      "    ls_model_name: 'gpt-3.5-turbo',\n",
      "    ls_model_type: 'chat',\n",
      "    ls_temperature: 1,\n",
      "    ls_max_tokens: undefined,\n",
      "    ls_stop: undefined\n",
      "  },\n",
      "  data: {\n",
      "    chunk: AIMessageChunk {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: '',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: [Object],\n",
      "      id: 'chatcmpl-9lOQhe44ip2q0DHfr0eYU9TF4mHtu',\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: [],\n",
      "      tool_call_chunks: [],\n",
      "      usage_metadata: undefined\n",
      "    }\n",
      "  }\n",
      "}\n",
      "{\n",
      "  event: 'on_chat_model_stream',\n",
      "  data: {\n",
      "    chunk: AIMessageChunk {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Sw',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: [Object],\n",
      "      id: 'chatcmpl-9lOQhe44ip2q0DHfr0eYU9TF4mHtu',\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: [],\n",
      "      tool_call_chunks: [],\n",
      "      usage_metadata: undefined\n",
      "    }\n",
      "  },\n",
      "  run_id: 'c9966059-70eb-4f24-9de3-2cf04320c8f6',\n",
      "  name: 'ChatOpenAI',\n",
      "  tags: [],\n",
      "  metadata: {\n",
      "    ls_provider: 'openai',\n",
      "    ls_model_name: 'gpt-3.5-turbo',\n",
      "    ls_model_type: 'chat',\n",
      "    ls_temperature: 1,\n",
      "    ls_max_tokens: undefined,\n",
      "    ls_stop: undefined\n",
      "  }\n",
      "}\n",
      "...Truncated\n"
     ]
    }
   ],
   "source": [
    "let idx = 0\n",
    "\n",
    "const stream = model.streamEvents(\n",
    "    \"Write me a 1 verse song about goldfish on the moon\",\n",
    "    {\n",
    "        version: \"v2\"\n",
    "    }\n",
    ");\n",
    "\n",
    "for await (const event of stream) {\n",
    "    idx += 1\n",
    "    if (idx === 5) {\n",
    "        console.log(\"...Truncated\");\n",
    "        break;\n",
    "    } \n",
    "    console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacb301",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now seen a few ways you can stream chat model responses.\n",
    "\n",
    "Next, check out this guide for more on [streaming with other LangChain modules](/docs/how_to/streaming)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
