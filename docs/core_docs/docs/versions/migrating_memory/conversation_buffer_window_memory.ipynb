{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8457ed-c0b1-4a74-abbd-9d3d2211270f",
   "metadata": {},
   "source": [
    "# Migrating off ConversationTokenBufferMemory\n",
    "\n",
    "Follow this guide if you're trying to migrate off one of the old memory classes listed below:\n",
    "\n",
    "\n",
    "| Memory Type                      | Description                                                                                                                                                       |\n",
    "|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `ConversationTokenBufferMemory`  | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n",
    "\n",
    "`ConversationTokenBufferMemory` applies additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model. \n",
    "\n",
    "This processing functionality can be accomplished using LangChain's built-in [trimMessages](https://api.js.langchain.com/functions/_langchain_core.messages.trimMessages.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79935247-acc7-4a05-a387-5d72c9c8c8cb",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    ":::important\n",
    "\n",
    "Weâ€™ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.\n",
    "\n",
    "While this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.\n",
    "\n",
    "More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.\n",
    "\n",
    "For instance, the LangGraph [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) demonstrates\n",
    "how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.\n",
    ":::\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f9459-9fb6-4942-99c9-64558aedd7d4",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "```{=mdx}\n",
    "import Npm2Yarn from \"@theme/Npm2Yarn\"\n",
    "\n",
    "<Npm2Yarn>\n",
    "  @langchain/openai @langchain/core zod\n",
    "</Npm2Yarn>\n",
    "```\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2d951",
   "metadata": {},
   "source": [
    "## Reimplementing ConversationTokenBufferMemory logic\n",
    "\n",
    "Here, we'll use `trimMessages` to keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.\n",
    "\n",
    "<details open> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1550bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  AIMessage,\n",
    "  HumanMessage,\n",
    "  SystemMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "\n",
    "const messages = [\n",
    "  new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "  new HumanMessage(\"i wonder why it's called langchain\"),\n",
    "  new AIMessage(\n",
    "    'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "  ),\n",
    "  new HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "  new AIMessage(\n",
    "      \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "  ),\n",
    "  new HumanMessage(\"why is 42 always the answer?\"),\n",
    "  new AIMessage(\n",
    "      \"Because it's the only number that's constantly right, even when it doesn't add up!\"\n",
    "  ),\n",
    "  new HumanMessage(\"What did the cow say?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6442f74b-2c36-48fd-a3d1-c7c5d18c050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemMessage {\n",
      "  \"content\": \"you're a good assistant, you always respond with a joke.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "HumanMessage {\n",
      "  \"content\": \"and who is harrison chasing anyways\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"content\": \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {},\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": []\n",
      "}\n",
      "HumanMessage {\n",
      "  \"content\": \"why is 42 always the answer?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"content\": \"Because it's the only number that's constantly right, even when it doesn't add up!\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {},\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": []\n",
      "}\n",
      "HumanMessage {\n",
      "  \"content\": \"What did the cow say?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { trimMessages } from \"@langchain/core/messages\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const selectedMessages = await trimMessages(\n",
    "  messages,\n",
    "  {\n",
    "    // Please see API reference for trimMessages for other ways to specify a token counter.\n",
    "    tokenCounter: new ChatOpenAI({ model: \"gpt-4o\" }),\n",
    "    maxTokens: 80,  // <-- token limit\n",
    "    // The startOn is specified\n",
    "    // to make sure we do not generate a sequence where\n",
    "    // a ToolMessage that contains the result of a tool invocation\n",
    "    // appears before the AIMessage that requested a tool invocation\n",
    "    // as this will cause some chat models to raise an error.\n",
    "    startOn: [\"human\", \"ai\"],\n",
    "    strategy: \"last\",\n",
    "    includeSystem: true,  // <-- Keep the system message\n",
    "  }\n",
    ")\n",
    "\n",
    "for (const msg of selectedMessages) {\n",
    "    console.log(msg);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05d272-2d22-44b7-9fa6-e617a48584b4",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Modern usage with LangGraph\n",
    "\n",
    "The example below shows how to use LangGraph to add simple conversation pre-processing logic.\n",
    "\n",
    "```{=mdx}\n",
    ":::note\n",
    "\n",
    "If you want to avoid running the computation on the entire conversation history each time, you can follow\n",
    "the [how-to guide on summarization](https://langchain-ai.github.io/langgraphjs/how-tos/add-summary-conversation-history/) that demonstrates\n",
    "how to discard older messages, ensuring they aren't re-processed during later turns.\n",
    "\n",
    ":::\n",
    "```\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d360e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! How can I help you today?\n",
      "Your name is Bob. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "import { v4 as uuidv4 } from 'uuid';\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StateGraph, MessagesAnnotation, END, START, MemorySaver } from \"@langchain/langgraph\";\n",
    "import { trimMessages } from \"@langchain/core/messages\";\n",
    "\n",
    "// Define a chat model\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "\n",
    "// Define the function that calls the model\n",
    "const callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => {\n",
    "  // highlight-start\n",
    "  const selectedMessages = await trimMessages(\n",
    "    state.messages,\n",
    "    {\n",
    "      tokenCounter: (messages) => messages.length, // Simple message count instead of token count\n",
    "      maxTokens: 5, // Allow up to 5 messages\n",
    "      strategy: \"last\",\n",
    "      startOn: [\"human\", \"ai\"],\n",
    "      includeSystem: true,\n",
    "      allowPartial: false,\n",
    "    }\n",
    "  );\n",
    "  // highlight-end\n",
    "\n",
    "  const response = await model.invoke(selectedMessages);\n",
    "\n",
    "  // With LangGraph, we're able to return a single message, and LangGraph will concatenate\n",
    "  // it to the existing list\n",
    "  return { messages: [response] };\n",
    "};\n",
    "\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph(MessagesAnnotation)\n",
    "// Define the two nodes we will cycle between\n",
    "  .addNode(\"model\", callModel)\n",
    "  .addEdge(START, \"model\")\n",
    "  .addEdge(\"model\", END)\n",
    "\n",
    "const app = workflow.compile({\n",
    "  // Adding memory is straightforward in LangGraph!\n",
    "  // Just pass a checkpointer to the compile method.\n",
    "  checkpointer: new MemorySaver()\n",
    "});\n",
    "\n",
    "// The thread id is a unique key that identifies this particular conversation\n",
    "// ---\n",
    "// NOTE: this must be `thread_id` and not `threadId` as the LangGraph internals expect `thread_id`\n",
    "// ---\n",
    "const thread_id = uuidv4();\n",
    "const config = { configurable: { thread_id } };\n",
    "\n",
    "const inputMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"hi! I'm bob\",\n",
    "}\n",
    "for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n",
    "  // LangGraph will stream events back inside objects where the key is the name of the node.\n",
    "  const key = Object.keys(event)[0];\n",
    "  if (key === \"model\") {\n",
    "    const lastMessage = event[key].messages[event[key].messages.length - 1];\n",
    "    console.log(lastMessage.content);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Here, let's confirm that the AI remembers our name!\n",
    "const followUpMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"what was my name?\",\n",
    "}\n",
    "\n",
    "// ---\n",
    "// NOTE: You must pass the same thread id to continue the conversation\n",
    "// we do that here by passing the same `config` object to the `.stream` call.\n",
    "// ---\n",
    "for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\n",
    "  const key = Object.keys(event)[0];\n",
    "  if (key === \"model\") {\n",
    "    const lastMessage = event[key].messages[event[key].messages.length - 1];\n",
    "    console.log(lastMessage.content);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84229e2e-a578-4b21-840a-814223406402",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Usage with a pre-built langgraph agent\n",
    "\n",
    "This example shows usage of an Agent Executor with a pre-built agent constructed using the [create_tool_calling_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) function.\n",
    "\n",
    "If you are using one of the [old LangChain pre-built agents](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able\n",
    "to replace that code with the new [langgraph pre-built agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) which leverages\n",
    "native tool calling capabilities of chat models and will likely work better out of the box.\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f671db87-8f01-453e-81fd-4e603140a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob. What is my age?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)\n",
      " Call ID: call_jsMvoIFv970DhqqLCJDzPKsp\n",
      "  Args:\n",
      "    name: bob\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_age\n",
      "\n",
      "42 years old\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bob, you are 42 years old.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "do you remember my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_age(name: str) -> str:\n",
    "    \"\"\"Use this tool to find the user's age.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if \"bob\" in name.lower():\n",
    "        return \"42 years old\"\n",
    "    return \"41 years old\"\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# highlight-start\n",
    "def state_modifier(state) -> list[BaseMessage]:\n",
    "    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n",
    "    # We're using the message processor defined above.\n",
    "    return trim_messages(\n",
    "        state[\"messages\"],\n",
    "        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
    "        max_tokens=5,  # <-- allow up to 5 messages.\n",
    "        strategy=\"last\",\n",
    "        # The start_on is specified\n",
    "        # to make sure we do not generate a sequence where\n",
    "        # a ToolMessage that contains the result of a tool invocation\n",
    "        # appears before the AIMessage that requested a tool invocation\n",
    "        # as this will cause some chat models to raise an error.\n",
    "        start_on=(\"human\", \"ai\"),\n",
    "        include_system=True,  # <-- Keep the system message\n",
    "        allow_partial=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# highlight-end\n",
    "\n",
    "app = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_user_age],\n",
    "    checkpointer=memory,\n",
    "    # highlight-next-line\n",
    "    state_modifier=state_modifier,\n",
    ")\n",
    "\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "# that it's capable of working like an agent.\n",
    "input_message = HumanMessage(content=\"hi! I'm bob. What is my age?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Confirm that the chat bot has access to previous conversation\n",
    "# and can respond to the user saying that the user's name is Bob.\n",
    "input_message = HumanMessage(content=\"do you remember my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e54ccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed to msgModifier [\n",
      "  HumanMessage {\n",
      "    \"id\": \"2f1d9e4a-043c-4ec2-9565-97bb939090c0\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  }\n",
      "]\n",
      "Returned from msgModifier [\n",
      "  HumanMessage {\n",
      "    \"id\": \"2f1d9e4a-043c-4ec2-9565-97bb939090c0\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  }\n",
      "]\n",
      "Passed to msgModifier [\n",
      "  HumanMessage {\n",
      "    \"id\": \"2f1d9e4a-043c-4ec2-9565-97bb939090c0\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JpfxFiYluLN33MKNgMIh4pSHQ1\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 57,\n",
      "        \"totalTokens\": 72\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"get_user_age\",\n",
      "        \"args\": {\n",
      "          \"input\": \"Bob\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 57,\n",
      "      \"output_tokens\": 15,\n",
      "      \"total_tokens\": 72\n",
      "    }\n",
      "  },\n",
      "  ToolMessage {\n",
      "    \"id\": \"3fbc61c3-e4ad-444b-af05-529dff46b865\",\n",
      "    \"content\": \"42 years old\",\n",
      "    \"name\": \"get_user_age\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_call_id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "  }\n",
      "]\n",
      "Returned from msgModifier [\n",
      "  HumanMessage {\n",
      "    \"id\": \"2f1d9e4a-043c-4ec2-9565-97bb939090c0\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JpfxFiYluLN33MKNgMIh4pSHQ1\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 57,\n",
      "        \"totalTokens\": 72\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"get_user_age\",\n",
      "        \"args\": {\n",
      "          \"input\": \"Bob\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 57,\n",
      "      \"output_tokens\": 15,\n",
      "      \"total_tokens\": 72\n",
      "    }\n",
      "  },\n",
      "  ToolMessage {\n",
      "    \"id\": \"3fbc61c3-e4ad-444b-af05-529dff46b865\",\n",
      "    \"content\": \"42 years old\",\n",
      "    \"name\": \"get_user_age\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_call_id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "  }\n",
      "]\n",
      "Passed to msgModifier [\n",
      "  ToolMessage {\n",
      "    \"id\": \"380cf075-2b2d-495f-b769-9d07fda53d61\",\n",
      "    \"content\": \"42 years old\",\n",
      "    \"name\": \"get_user_age\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_call_id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JpfxFiYluLN33MKNgMIh4pSHQ1\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 57,\n",
      "        \"totalTokens\": 72\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"get_user_age\",\n",
      "        \"args\": {\n",
      "          \"input\": \"Bob\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": []\n",
      "  },\n",
      "  HumanMessage {\n",
      "    \"id\": \"10103233-8bc1-4425-abf0-eb7c8bbc137b\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JqQ5Hh4T4pkErMGhnMYOkgppZ2\",\n",
      "    \"content\": \"Hi Bob! You are 42 years old.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 11,\n",
      "        \"promptTokens\": 84,\n",
      "        \"totalTokens\": 95\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  },\n",
      "  HumanMessage {\n",
      "    \"id\": \"43428578-dba5-475d-ab5d-2b6f2e944e5e\",\n",
      "    \"content\": \"do you remember my name?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  }\n",
      "]\n",
      "Returned from msgModifier [\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JpfxFiYluLN33MKNgMIh4pSHQ1\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 57,\n",
      "        \"totalTokens\": 72\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"get_user_age\",\n",
      "        \"args\": {\n",
      "          \"input\": \"Bob\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_KAispSTkEY5VC4apLZK4vQtB\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": []\n",
      "  },\n",
      "  HumanMessage {\n",
      "    \"id\": \"10103233-8bc1-4425-abf0-eb7c8bbc137b\",\n",
      "    \"content\": \"hi! I'm bob. What is my age?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"chatcmpl-AB6JqQ5Hh4T4pkErMGhnMYOkgppZ2\",\n",
      "    \"content\": \"Hi Bob! You are 42 years old.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 11,\n",
      "        \"promptTokens\": 84,\n",
      "        \"totalTokens\": 95\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"system_fingerprint\": \"fp_e375328146\"\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  },\n",
      "  HumanMessage {\n",
      "    \"id\": \"43428578-dba5-475d-ab5d-2b6f2e944e5e\",\n",
      "    \"content\": \"do you remember my name?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BadRequestError: 400 An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_KAispSTkEY5VC4apLZK4vQtB\n",
      "    at APIError.generate (/Users/bracesproul/code/lang-chain-ai/langchainjs/libs/langchain-openai/node_modules/openai/error.js:45:20)\n",
      "    at OpenAI.makeStatusError (/Users/bracesproul/code/lang-chain-ai/langchainjs/libs/langchain-openai/node_modules/openai/core.js:275:33)\n",
      "    at OpenAI.makeRequest (/Users/bracesproul/code/lang-chain-ai/langchainjs/libs/langchain-openai/node_modules/openai/core.js:318:30)\n",
      "    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n",
      "    at async /Users/bracesproul/code/lang-chain-ai/langchainjs/libs/langchain-openai/dist/chat_models.cjs:1362:29\n",
      "    at async RetryOperation._fn (/Users/bracesproul/code/lang-chain-ai/langchainjs/node_modules/p-retry/index.js:50:12) {\n",
      "  status: 400,\n",
      "  headers: {\n",
      "    'access-control-expose-headers': 'X-Request-ID',\n",
      "    'alt-svc': 'h3=\":443\"; ma=86400',\n",
      "    'cf-cache-status': 'DYNAMIC',\n",
      "    'cf-ray': '8c857a870f55964a-SJC',\n",
      "    connection: 'keep-alive',\n",
      "    'content-length': '324',\n",
      "    'content-type': 'application/json',\n",
      "    date: 'Tue, 24 Sep 2024 20:21:03 GMT',\n",
      "    'openai-organization': 'langchain',\n",
      "    'openai-processing-ms': '15',\n",
      "    'openai-version': '2020-10-01',\n",
      "    server: 'cloudflare',\n",
      "    'set-cookie': '__cf_bm=I9Ltd9I3J.qXmy5026JLyzzROeUBA9fjyxAqHIz.kEw-1727209263-1.0.1.1-eqgxByv3.cFt.njvOQMVEnZpVoCWPgWuTuph6S1J0vGpAKBox4uPkMjpH7YuKh1gSW_fiquUs8zFS.qHVryALw; path=/; expires=Tue, 24-Sep-24 20:51:03 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=wkD6_JbkII6mginJpjY_126mXGbx43QsYbuh9D2V38A-1727209263714-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n",
      "    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n",
      "    'x-content-type-options': 'nosniff',\n",
      "    'x-ratelimit-limit-requests': '10000',\n",
      "    'x-ratelimit-limit-tokens': '30000000',\n",
      "    'x-ratelimit-remaining-requests': '9999',\n",
      "    'x-ratelimit-remaining-tokens': '29999958',\n",
      "    'x-ratelimit-reset-requests': '6ms',\n",
      "    'x-ratelimit-reset-tokens': '0s',\n",
      "    'x-request-id': 'req_afb20a8716d6a96e91f1def84b9dcddd'\n",
      "  },\n",
      "  request_id: 'req_afb20a8716d6a96e91f1def84b9dcddd',\n",
      "  error: {\n",
      "    message: \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_KAispSTkEY5VC4apLZK4vQtB\",\n",
      "    type: 'invalid_request_error',\n",
      "    param: 'messages.[1].role',\n",
      "    code: null\n",
      "  },\n",
      "  code: null,\n",
      "  param: 'messages.[1].role',\n",
      "  type: 'invalid_request_error',\n",
      "  attemptNumber: 1,\n",
      "  retriesLeft: 6,\n",
      "  pregelTaskId: '688d682d-2711-5dc4-a593-1f374c23de45'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "import { v4 as uuidv4 } from 'uuid';\n",
    "import { BaseMessage, trimMessages } from \"@langchain/core/messages\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const getUserAge = tool(\n",
    "  (name: string): string => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    if (name.toLowerCase().includes(\"bob\")) {\n",
    "      return \"42 years old\";\n",
    "    }\n",
    "    return \"41 years old\";\n",
    "  },\n",
    "  {\n",
    "    name: \"get_user_age\",\n",
    "    description: \"Use this tool to find the user's age.\",\n",
    "    schema: z.string().describe(\"the name of the user\"),\n",
    "  }\n",
    ");\n",
    "\n",
    "const memory = new MemorySaver();\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "\n",
    "// highlight-start\n",
    "const stateModifier = async (messages: BaseMessage[]): Promise<BaseMessage[]> => {\n",
    "  // We're using the message processor defined above.\n",
    "  return trimMessages(\n",
    "    messages,\n",
    "    {\n",
    "      tokenCounter: (msgs) => msgs.length, // <-- .length will simply count the number of messages rather than tokens\n",
    "      maxTokens: 5, // <-- allow up to 5 messages.\n",
    "      strategy: \"last\",\n",
    "      // The startOn is specified\n",
    "      // to make sure we do not generate a sequence where\n",
    "      // a ToolMessage that contains the result of a tool invocation\n",
    "      // appears before the AIMessage that requested a tool invocation\n",
    "      // as this will cause some chat models to raise an error.\n",
    "      startOn: [\"human\", \"ai\"],\n",
    "      includeSystem: true, // <-- Keep the system message\n",
    "      allowPartial: false,\n",
    "    }\n",
    "  );\n",
    "};\n",
    "// highlight-end\n",
    "\n",
    "const app = createReactAgent({\n",
    "  llm: model,\n",
    "  tools: [getUserAge],\n",
    "  checkpointSaver: memory,\n",
    "  // highlight-next-line\n",
    "  messageModifier: async (msgs) => {\n",
    "    console.log(\"Passed to msgModifier\", msgs);\n",
    "    const modified = await stateModifier(msgs);\n",
    "    console.log(\"Returned from msgModifier\", modified);\n",
    "    return modified;\n",
    "  },\n",
    "});\n",
    "\n",
    "// The thread id is a unique key that identifies\n",
    "// this particular conversation.\n",
    "// We'll just generate a random uuid here.\n",
    "const threadId = uuidv4();\n",
    "const config = { configurable: { thread_id: threadId } };\n",
    "\n",
    "// Tell the AI that our name is Bob, and ask it to use a tool to confirm\n",
    "// that it's capable of working like an agent.\n",
    "const inputMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"hi! I'm bob. What is my age?\",\n",
    "}\n",
    "\n",
    "for await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n",
    "  const key = Object.keys(event)[0];\n",
    "  if (event[key] && \"messages\" in event[key]) {\n",
    "    const lastMessage = event[key].messages[event[key].messages.length - 1];\n",
    "    // console.log(lastMessage.content);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Confirm that the chat bot has access to previous conversation\n",
    "// and can respond to the user saying that the user's name is Bob.\n",
    "const followUpMessage = {\n",
    "  role: \"user\",\n",
    "  content: \"do you remember my name?\",\n",
    "};\n",
    "\n",
    "for await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\n",
    "  const key = Object.keys(event)[0];\n",
    "  if (event[key] && \"messages\" in event[key]) {\n",
    "    const lastMessage = event[key].messages[event[key].messages.length - 1];\n",
    "    // console.log(lastMessage.content);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d16e09-1d90-4153-8576-6d3996cb5a6c",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## LCEL: Add a preprocessing step\n",
    "\n",
    "The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.\n",
    "\n",
    "This approach is conceptually simple and will work in many situations; for example, if using a [RunnableWithMessageHistory](/docs/how_to/message_history/) instead of wrapping the chat model, wrap the chat model with the pre-processor.\n",
    "\n",
    "The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:\n",
    "\n",
    "1. As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).\n",
    "2. The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.\n",
    "\n",
    ":::caution\n",
    "\n",
    "If you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!\n",
    "\n",
    ":::\n",
    "\n",
    "<details open>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072046bb-3892-4206-8ae5-025e93110dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  what_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)\n",
      " Call ID: call_urHTB5CShhcKz37QiVzNBlIS\n",
      "  Args:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "@tool\n",
    "def what_did_the_cow_say() -> str:\n",
    "    \"\"\"Check to see what the cow said.\"\"\"\n",
    "    return \"foo\"\n",
    "\n",
    "\n",
    "# highlight-start\n",
    "message_processor = trim_messages(  # Returns a Runnable if no messages are provided\n",
    "    token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
    "    max_tokens=5,  # <-- allow up to 5 messages.\n",
    "    strategy=\"last\",\n",
    "    # The start_on is specified\n",
    "    # to make sure we do not generate a sequence where\n",
    "    # a ToolMessage that contains the result of a tool invocation\n",
    "    # appears before the AIMessage that requested a tool invocation\n",
    "    # as this will cause some chat models to raise an error.\n",
    "    start_on=(\"human\", \"ai\"),\n",
    "    include_system=True,  # <-- Keep the system message\n",
    "    allow_partial=False,\n",
    ")\n",
    "# highlight-end\n",
    "\n",
    "# Note that we bind tools to the model first!\n",
    "model_with_tools = model.bind_tools([what_did_the_cow_say])\n",
    "\n",
    "# highlight-next-line\n",
    "model_with_preprocessor = message_processor | model_with_tools\n",
    "\n",
    "full_history = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"why is 42 always the answer?\"),\n",
    "    AIMessage(\n",
    "        \"Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!\"\n",
    "    ),\n",
    "    HumanMessage(\"What did the cow say?\"),\n",
    "]\n",
    "\n",
    "\n",
    "# We pass it explicity to the model_with_preprocesor for illustrative purposes.\n",
    "# If you're using `RunnableWithMessageHistory` the history will be automatically\n",
    "# read from the source the you configure.\n",
    "model_with_preprocessor.invoke(full_history).pretty_print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5da7225a-5e94-4f53-bb0d-86b6b528d150",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "If you need to implement more efficient logic and want to use `RunnableWithMessageHistory` for now the way to achieve this\n",
    "is to subclass from [BaseChatMessageHistory](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) and\n",
    "define appropriate logic for `add_messages` (that doesn't simply append the history, but instead re-writes it).\n",
    "\n",
    "Unless you have a good reason to implement this solution, you should instead use LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717810",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Explore persistence with LangGraph:\n",
    "\n",
    "* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "* [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "* [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\n",
    "* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\n",
    "\n",
    "Add persistence with simple LCEL (favor langgraph for more complex use cases):\n",
    "\n",
    "* [How to add message history](/docs/how_to/message_history/)\n",
    "\n",
    "Working with message history:\n",
    "\n",
    "* [How to trim messages](/docs/how_to/trim_messages)\n",
    "* [How to filter messages](/docs/how_to/filter_messages/)\n",
    "* [How to merge message runs](/docs/how_to/merge_message_runs/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
