{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: ChatOpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatOpenAI\n",
    "\n",
    "This will help you getting started with ChatOpenAI [chat models](/docs/concepts/#chat-models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | [PY support](https:/python.langchain.com/v0.2/docs/integrations/chat/openai) | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "- TODO: Update with relevant info.\n",
    "\n",
    "To access ChatOpenAI models you'll need to create a ChatOpenAI account, get an API key, and install the `@langchain/openai` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "- TODO: Update with relevant info.\n",
    "\n",
    "Head to [OpenAI's website](https://platform.openai.com/) to sign up to ChatOpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash\n",
    "# export LANGCHAIN_TRACING_V2=\"true\"\n",
    "# export LANGCHAIN_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain ChatOpenAI integration lives in the `@langchain/openai` package:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash npm2yarn\n",
    "npm i @langchain/openai\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\" \n",
    "\n",
    "const llm = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "    maxTokens: undefined,\n",
    "    timeout: undefined,\n",
    "    maxRetries: 2,\n",
    "    // other params...\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoATPJseCeF0xADWYOeVKgNKPqdJ\",\n",
      "  \"content\": \"J'adore la programmation.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 5,\n",
      "      \"promptTokens\": 31,\n",
      "      \"totalTokens\": 36\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_4e2b2da518\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 31,\n",
      "    \"output_tokens\": 5,\n",
      "    \"total_tokens\": 36\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const aiMsg = await llm.invoke([\n",
    "    [\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ],\n",
    "    [\"human\", \"I love programming.\"],\n",
    "])\n",
    "aiMsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "console.log(aiMsg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:\n",
    "\n",
    "- TODO: Run cells so output can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoAUdg1rAZhsYnwirmpUlGRH8033\",\n",
      "  \"content\": \"Ich liebe das Programmieren.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 6,\n",
      "      \"promptTokens\": 26,\n",
      "      \"totalTokens\": 32\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_4e2b2da518\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 26,\n",
      "    \"output_tokens\": 6,\n",
      "    \"total_tokens\": 32\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\"\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "    [\n",
    "        [\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ],\n",
    "        [\"human\", \"{input}\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "const chain = prompt.pipe(llm);\n",
    "await chain.invoke(\n",
    "    {\n",
    "        input_language: \"English\",\n",
    "        output_language: \"German\",\n",
    "        input: \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee55bc-ffc8-4cfa-801c-993953a08cfd",
   "metadata": {},
   "source": [
    "## Multimodal messages\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::info\n",
    "This feature is currently in preview. The message schema may change in future releases.\n",
    ":::\n",
    "\n",
    "```\n",
    "\n",
    "OpenAI supports interleaving images with text in input messages with their `gpt-4-vision-preview`. Here's an example of how this looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd55c000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoAVJVZOVYSAvRDpQXVnDF0xIStb\",\n",
      "  \"content\": \"The image shows a classic hot dog, consisting of a grilled or steamed sausage served in the slit of a partially sliced bun. The sausage appears to have grill marks, indicating it may have been cooked on a grill. There are no visible condiments or additional toppings in this image; it's a simple and basic presentation of a hot dog.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 69,\n",
      "      \"promptTokens\": 438,\n",
      "      \"totalTokens\": 507\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 438,\n",
      "    \"output_tokens\": 69,\n",
      "    \"total_tokens\": 507\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import * as fs from \"node:fs/promises\";\n",
    "\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const imageData = await fs.readFile(\"../../../../../examples/hotdog.jpg\");\n",
    "const chat = new ChatOpenAI({\n",
    "  model: \"gpt-4-vision-preview\",\n",
    "  maxTokens: 1024,\n",
    "});\n",
    "const message = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"What's in this image?\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url: {\n",
    "        url: `data:image/jpeg;base64,${imageData.toString(\"base64\")}`,\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "const res = await chat.invoke([message]);\n",
    "console.log(res);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eafbba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoAZXqa3VGSRx4pY6frLAo7NbAqq\",\n",
      "  \"content\": \"The image contains the text \\\"LangChain\\\" and features a graphic of a parrot to the left and two interlinked rings to the right of the text.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 32,\n",
      "      \"promptTokens\": 778,\n",
      "      \"totalTokens\": 810\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 778,\n",
      "    \"output_tokens\": 32,\n",
      "    \"total_tokens\": 810\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const hostedImageMessage = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"What does this image say?\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url:\n",
    "        \"https://www.freecodecamp.org/news/content/images/2023/05/Screenshot-2023-05-29-at-5.40.38-PM.png\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "const res2 = await chat.invoke([hostedImageMessage]);\n",
    "console.log(res2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3832fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoAb7AIDkgfO5VzJ8lY4h3Rxose7\",\n",
      "  \"content\": \"This image appears to be a screenshot of a user interface for a digital assistant or search tool named \\\"WebLangChain,\\\" indicated to be powered by \\\"Twirly.\\\" The interface has a dark theme with a prompt that says, \\\"Ask me anything about anything!\\\" Below the prompt, there is an input field where users can type their queries. Suggested questions are provided below this field, including \\\"what is langchain?\\\", \\\"history of mesopotamia\\\", \\\"how to build a discord bot\\\", \\\"leonardo dicaprio girlfriend\\\", \\\"fun gift ideas for software engineers\\\", \\\"how does a prism separate light\\\", and \\\"what beer is best.\\\" These sample questions suggest the assistant can answer a wide range of topics. On the right side of the input box, there is a send button shaped like a blue speech bubble with a white arrow inside, indicating the action to submit a question.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 180,\n",
      "      \"promptTokens\": 101,\n",
      "      \"totalTokens\": 281\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 101,\n",
      "    \"output_tokens\": 180,\n",
      "    \"total_tokens\": 281\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const lowDetailImage = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"Summarize the contents of this image.\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url: {\n",
    "        url: \"https://blog.langchain.dev/content/images/size/w1248/format/webp/2023/10/Screenshot-2023-10-03-at-4.55.29-PM.png\",\n",
    "        detail: \"low\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "const res3 = await chat.invoke([lowDetailImage]);\n",
    "console.log(res3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39ecb3",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "\n",
    "OpenAI chat models support calling multiple functions to get all required data to answer a question.\n",
    "Here's an example how a conversation turn with this functionality might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c65f489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'San Francisco, CA', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_XVerHEcCvaXpdPObWM4rLIkQ'\n",
      "  },\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'Tokyo, Japan', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_p0QzmXrnfL9Jiw52WbzESoSs'\n",
      "  },\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'Paris, France', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_6ka7piwYmAtvsFnwgXbfHtK6'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ToolMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// Mocked out function, could be a database/API call in production\n",
    "function getCurrentWeather(location: string, _unit?: string) {\n",
    "  if (location.toLowerCase().includes(\"tokyo\")) {\n",
    "    return JSON.stringify({ location, temperature: \"10\", unit: \"celsius\" });\n",
    "  } else if (location.toLowerCase().includes(\"san francisco\")) {\n",
    "    return JSON.stringify({\n",
    "      location,\n",
    "      temperature: \"72\",\n",
    "      unit: \"fahrenheit\",\n",
    "    });\n",
    "  } else {\n",
    "    return JSON.stringify({ location, temperature: \"22\", unit: \"celsius\" });\n",
    "  }\n",
    "}\n",
    "\n",
    "// Bind function to the model as a tool\n",
    "const chat = new ChatOpenAI({\n",
    "  model: \"gpt-3.5-turbo-1106\",\n",
    "  maxTokens: 128,\n",
    "}).bind({\n",
    "  tools: [\n",
    "    {\n",
    "      type: \"function\",\n",
    "      function: {\n",
    "        name: \"get_current_weather\",\n",
    "        description: \"Get the current weather in a given location\",\n",
    "        parameters: {\n",
    "          type: \"object\",\n",
    "          properties: {\n",
    "            location: {\n",
    "              type: \"string\",\n",
    "              description: \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            unit: { type: \"string\", enum: [\"celsius\", \"fahrenheit\"] },\n",
    "          },\n",
    "          required: [\"location\"],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "  tool_choice: \"auto\",\n",
    "});\n",
    "\n",
    "// Ask initial question that requires multiple tool calls\n",
    "const res = await chat.invoke([\n",
    "  [\"human\", \"What's the weather like in San Francisco, Tokyo, and Paris?\"],\n",
    "]);\n",
    "console.log(res.tool_calls);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0d3a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9qoAjNgsOegV4rkLkL0nYyXJEvwV0\",\n",
      "  \"content\": \"The current weather in:\\n- San Francisco, CA is 72°F\\n- Tokyo, Japan is 10°C\\n- Paris, France is 22°C\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 33,\n",
      "      \"promptTokens\": 248,\n",
      "      \"totalTokens\": 281\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_adbef9f124\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 248,\n",
      "    \"output_tokens\": 33,\n",
      "    \"total_tokens\": 281\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "// Format the results from calling the tool calls back to OpenAI as ToolMessages\n",
    "const toolMessages = res.additional_kwargs.tool_calls?.map((toolCall) => {\n",
    "  const toolCallResult = getCurrentWeather(\n",
    "    JSON.parse(toolCall.function.arguments).location\n",
    "  );\n",
    "  return new ToolMessage({\n",
    "    tool_call_id: toolCall.id,\n",
    "    name: toolCall.function.name,\n",
    "    content: toolCallResult,\n",
    "  });\n",
    "});\n",
    "\n",
    "// Send the results back as the next step in the conversation\n",
    "const finalResponse = await chat.invoke([\n",
    "  [\"human\", \"What's the weather like in San Francisco, Tokyo, and Paris?\"],\n",
    "  res,\n",
    "  ...(toolMessages ?? []),\n",
    "]);\n",
    "\n",
    "console.log(finalResponse);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067715fe",
   "metadata": {},
   "source": [
    "### `.withStructuredOutput({ ... })`\n",
    "\n",
    "You can also use the `.withStructuredOutput({ ... })` method to coerce `ChatOpenAI` into returning a structured output.\n",
    "\n",
    "The method allows for passing in either a Zod object, or a valid JSON schema (like what is returned from [`zodToJsonSchema`](https://www.npmjs.com/package/zod-to-json-schema)).\n",
    "\n",
    "Using the method is simple. Just define your LLM and call `.withStructuredOutput({ ... })` on it, passing the desired schema.\n",
    "\n",
    "Here is an example using a Zod schema and the `functionCalling` mode (default mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94bab2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ operation: 'add', number1: 2, number2: 2 }\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4-turbo-preview\",\n",
    "});\n",
    "\n",
    "const calculatorSchema = z.object({\n",
    "  operation: z.enum([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n",
    "  number1: z.number(),\n",
    "  number2: z.number(),\n",
    "});\n",
    "\n",
    "const modelWithStructuredOutput = model.withStructuredOutput(calculatorSchema);\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are VERY bad at math and must always use a calculator.\"],\n",
    "  [\"human\", \"Please help me!! What is 2 + 2?\"],\n",
    "]);\n",
    "const chain = prompt.pipe(modelWithStructuredOutput);\n",
    "const result = await chain.invoke({});\n",
    "console.log(result);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e97547",
   "metadata": {},
   "source": [
    "You can also specify `includeRaw` to return the parsed and raw output in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640acaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  raw: AIMessage {\n",
      "    \"id\": \"chatcmpl-9qoAl49McgKC5htc6M8fu4dGkavHn\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_zO1pNCvFXBFjXYpR7U3sC1Hq\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 93,\n",
      "        \"totalTokens\": 108\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"calculator\",\n",
      "        \"args\": {\n",
      "          \"operation\": \"add\",\n",
      "          \"number1\": 2,\n",
      "          \"number2\": 2\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_zO1pNCvFXBFjXYpR7U3sC1Hq\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 93,\n",
      "      \"output_tokens\": 15,\n",
      "      \"total_tokens\": 108\n",
      "    }\n",
      "  },\n",
      "  parsed: { operation: 'add', number1: 2, number2: 2 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const includeRawModel = model.withStructuredOutput(calculatorSchema, {\n",
    "  name: \"calculator\",\n",
    "  includeRaw: true,\n",
    "});\n",
    "\n",
    "const includeRawChain = prompt.pipe(includeRawModel);\n",
    "const includeRawResult = await includeRawChain.invoke({});\n",
    "console.log(includeRawResult);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f236c",
   "metadata": {},
   "source": [
    "Additionally, you can pass in an OpenAI function definition or JSON schema directly:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::info\n",
    "If using `jsonMode` as the `method` you must include context in your prompt about the structured output you want. This _must_ include the keyword: `JSON`.\n",
    ":::\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02e01d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ number1: 2, number2: 2, operation: 'add' }\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4-turbo-preview\",\n",
    "});\n",
    "\n",
    "const calculatorSchema = {\n",
    "  type: \"object\",\n",
    "  properties: {\n",
    "    operation: {\n",
    "      type: \"string\",\n",
    "      enum: [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "    },\n",
    "    number1: { type: \"number\" },\n",
    "    number2: { type: \"number\" },\n",
    "  },\n",
    "  required: [\"operation\", \"number1\", \"number2\"],\n",
    "};\n",
    "\n",
    "// Default mode is \"functionCalling\"\n",
    "const modelWithStructuredOutput = model.withStructuredOutput(calculatorSchema);\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are VERY bad at math and must always use a calculator.\n",
    "Respond with a JSON object containing three keys:\n",
    "'operation': the type of operation to execute, either 'add', 'subtract', 'multiply' or 'divide',\n",
    "'number1': the first number to operate on,\n",
    "'number2': the second number to operate on.\n",
    "`,\n",
    "  ],\n",
    "  [\"human\", \"Please help me!! What is 2 + 2?\"],\n",
    "]);\n",
    "const chain = prompt.pipe(modelWithStructuredOutput);\n",
    "const result = await chain.invoke({});\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae798f49",
   "metadata": {},
   "source": [
    "You can also specify 'includeRaw' to return the parsed and raw output in the result, as well as a \"name\" field to give the LLM additional context as to what you are generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a5579e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  raw: AIMessage {\n",
      "    \"id\": \"chatcmpl-9qoAnMHNXbmxETthVRVKR5txyRQ4s\",\n",
      "    \"content\": \"{\\n  \\\"operation\\\": \\\"add\\\",\\n  \\\"number1\\\": 2,\\n  \\\"number2\\\": 2\\n}\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 25,\n",
      "        \"promptTokens\": 91,\n",
      "        \"totalTokens\": 116\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 91,\n",
      "      \"output_tokens\": 25,\n",
      "      \"total_tokens\": 116\n",
      "    }\n",
      "  },\n",
      "  parsed: { operation: 'add', number1: 2, number2: 2 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const includeRawModel = model.withStructuredOutput(calculatorSchema, {\n",
    "  name: \"calculator\",\n",
    "  includeRaw: true,\n",
    "  method: \"jsonMode\",\n",
    "});\n",
    "\n",
    "const includeRawChain = prompt.pipe(includeRawModel);\n",
    "const includeRawResult = await includeRawChain.invoke({});\n",
    "console.log(includeRawResult);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf343e65",
   "metadata": {},
   "source": [
    "### Disabling parallel tool calls\n",
    "\n",
    "If you have multiple tools bound to the model, but you'd only like for a single tool to be called at a time, you can pass the `parallel_tool_calls` call option to enable/disable this behavior.\n",
    "By default, `parallel_tool_calls` is set to `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cb759f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4o\",\n",
    "});\n",
    "\n",
    "// Define your tools\n",
    "const calculatorSchema = z\n",
    "  .object({\n",
    "    operation: z.enum([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n",
    "    number1: z.number(),\n",
    "    number2: z.number(),\n",
    "  })\n",
    "  .describe(\"A tool to perform basic arithmetic operations\");\n",
    "const weatherSchema = z\n",
    "  .object({\n",
    "    city: z.string(),\n",
    "  })\n",
    "  .describe(\"A tool to get the weather in a city\");\n",
    "\n",
    "// Bind tools to the model\n",
    "const modelWithTools = model.bindTools([\n",
    "  {\n",
    "    type: \"function\",\n",
    "    function: {\n",
    "      name: \"calculator\",\n",
    "      description: calculatorSchema.description,\n",
    "      parameters: zodToJsonSchema(calculatorSchema),\n",
    "    },\n",
    "  },\n",
    "  {\n",
    "    type: \"function\",\n",
    "    function: {\n",
    "      name: \"weather\",\n",
    "      description: weatherSchema.description,\n",
    "      parameters: zodToJsonSchema(weatherSchema),\n",
    "    },\n",
    "  },\n",
    "]);\n",
    "\n",
    "// Invoke the model with `parallel_tool_calls` set to `true`\n",
    "const response = await modelWithTools.invoke(\n",
    "  [\"What is the weather in san francisco and what is 23716 times 27342?\"],\n",
    "  {\n",
    "    parallel_tool_calls: true,\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a69645",
   "metadata": {},
   "source": [
    "We can see it called two tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95db614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'weather',\n",
      "    args: { city: 'san francisco' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_aMhRLnzYEaotQjJODwi2i47B'\n",
      "  },\n",
      "  {\n",
      "    name: 'calculator',\n",
      "    args: { operation: 'multiply', number1: 23716, number2: 27342 },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_DzAjEgoYfXj34groG4ED6W5w'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "console.log(response.tool_calls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46a7bb",
   "metadata": {},
   "source": [
    "Invoke the model with `parallel_tool_calls` set to `false`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a2bac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "const response2 = await modelWithTools.invoke(\n",
    "  [\"What is the weather in san francisco and what is 23716 times 27342?\"],\n",
    "  {\n",
    "    parallel_tool_calls: false,\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d91e9f",
   "metadata": {},
   "source": [
    "We can see it called one tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5731d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'weather',\n",
      "    args: { city: 'san francisco' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_U9r7WbJQCDoaJ8nFH7KUcF2n'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "console.log(response2.tool_calls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffc86b",
   "metadata": {},
   "source": [
    "## Custom URLs\n",
    "\n",
    "You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a092b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0.9,\n",
    "  configuration: {\n",
    "    baseURL: \"https://your_custom_url.com\",\n",
    "  },\n",
    "});\n",
    "\n",
    "const message = await model.invoke(\"Hi there!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b6300",
   "metadata": {},
   "source": [
    "You can also pass other `ClientOptions` parameters accepted by the official SDK.\n",
    "\n",
    "If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/chat/azure).\n",
    "\n",
    "## Calling fine-tuned models\n",
    "\n",
    "You can call fine-tuned OpenAI models by passing in your corresponding `modelName` parameter.\n",
    "\n",
    "This generally takes the form of `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  temperature: 0.9,\n",
    "  model: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n",
    "});\n",
    "\n",
    "const message = await model.invoke(\"Hi there!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2270901",
   "metadata": {},
   "source": [
    "## Generation metadata\n",
    "\n",
    "If you need additional information like logprobs or token usage, these will be returned directly in the `.invoke` response.\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::tip\n",
    "Requires `@langchain/core` version >=0.1.48.\n",
    ":::\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b675330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.00047559434,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.00004429897,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.00003166338,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.00062582217,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -9.0883464e-7,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.12731916,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -5.5122365e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000049663133,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.000010564331,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// See https://cookbook.openai.com/examples/using_logprobs for details\n",
    "const model = new ChatOpenAI({\n",
    "  logprobs: true,\n",
    "  // topLogprobs: 5,\n",
    "});\n",
    "\n",
    "const responseMessage = await model.invoke(\"Hi there!\");\n",
    "console.dir(responseMessage.response_metadata.logprobs, { depth: null });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c364f",
   "metadata": {},
   "source": [
    "### With callbacks\n",
    "\n",
    "You can also use the callbacks system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01e74121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.0004585519,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.000049305523,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.000029517714,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.00073185476,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -9.0883464e-7,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.104538105,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -6.704273e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000052643223,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.000012590794,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// See https://cookbook.openai.com/examples/using_logprobs for details\n",
    "const model = new ChatOpenAI({\n",
    "  logprobs: true,\n",
    "  // topLogprobs: 5,\n",
    "});\n",
    "\n",
    "const result = await model.invoke(\"Hi there!\", {\n",
    "  callbacks: [\n",
    "    {\n",
    "      handleLLMEnd(output) {\n",
    "        console.dir(output.generations[0][0].generationInfo.logprobs, { depth: null });\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f9f01aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.0004585519,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.000049305523,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.000029517714,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.00073185476,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -9.0883464e-7,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.104538105,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -6.704273e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000052643223,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.000012590794,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "console.dir(result.response_metadata.logprobs, { depth: null });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194627d",
   "metadata": {},
   "source": [
    "## Streaming tokens\n",
    "\n",
    "OpenAI supports streaming token counts via an opt-in call option. This can be set by passing `{ stream_options: { include_usage: true } }`.\n",
    "Setting this call option will cause the model to return an additional chunk at the end of the stream, containing the token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6efaebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ input_tokens: 13, output_tokens: 39, total_tokens: 52 }\n"
     ]
    }
   ],
   "source": [
    "import type { AIMessageChunk } from \"@langchain/core/messages\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { concat } from \"@langchain/core/utils/stream\";\n",
    "\n",
    "// Instantiate the model\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "const response = await model.stream(\"Hello, how are you?\", {\n",
    "  // Pass the stream options\n",
    "  stream_options: {\n",
    "    include_usage: true,\n",
    "  },\n",
    "});\n",
    "\n",
    "// Iterate over the response, only saving the last chunk\n",
    "let finalResult: AIMessageChunk | undefined;\n",
    "for await (const chunk of response) {\n",
    "  if (finalResult) {\n",
    "    finalResult = concat(finalResult, chunk);\n",
    "  } else {\n",
    "    finalResult = chunk;\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(finalResult?.usage_metadata);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all ChatOpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
