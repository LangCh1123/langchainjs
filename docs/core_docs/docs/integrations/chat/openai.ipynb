{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afaf8039",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: ChatOpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f1e0d",
   "metadata": {},
   "source": [
    "# ChatOpenAI\n",
    "\n",
    "This will help you getting started with ChatOpenAI [chat models](/docs/concepts/#chat-models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).\n",
    "\n",
    "## Overview\n",
    "### Integration details\n",
    "\n",
    "| Class | Package | Local | Serializable | [PY support](https:/python.langchain.com/v0.2/docs/integrations/chat/openai) | Package downloads | Package latest |\n",
    "| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\n",
    "| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) | ❌ | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square&label=%20) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square&label=%20) |\n",
    "\n",
    "### Model features\n",
    "| [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | [Image input](/docs/how_to/multimodal_inputs/) | Audio input | Video input | [Token-level streaming](/docs/how_to/chat_streaming/) | [Token usage](/docs/how_to/chat_token_usage_tracking/) | [Logprobs](/docs/how_to/logprobs/) |\n",
    "| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |\n",
    "| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | \n",
    "\n",
    "## Setup\n",
    "\n",
    "- TODO: Update with relevant info.\n",
    "\n",
    "To access ChatOpenAI models you'll need to create a ChatOpenAI account, get an API key, and install the `@langchain/openai` integration package.\n",
    "\n",
    "### Credentials\n",
    "\n",
    "- TODO: Update with relevant info.\n",
    "\n",
    "Head to [OpenAI's website](https://platform.openai.com/) to sign up to ChatOpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee0c4b-9764-423a-9dbf-95129e185210",
   "metadata": {},
   "source": [
    "If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash\n",
    "# export LANGCHAIN_TRACING_V2=\"true\"\n",
    "# export LANGCHAIN_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain ChatOpenAI integration lives in the `@langchain/openai` package:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    "```bash npm2yarn\n",
    "npm i @langchain/openai\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\" \n",
    "\n",
    "const llm = new ChatOpenAI({\n",
    "    model: \"gpt-4o\",\n",
    "    temperature: 0,\n",
    "    maxTokens: undefined,\n",
    "    timeout: undefined,\n",
    "    maxRetries: 2,\n",
    "    // other params...\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {},
   "source": [
    "## Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e0dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB4GvhlRb0x3hxupLBQYOKKmTxvV\",\n",
      "  \"content\": \"J'adore la programmation.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 8,\n",
      "      \"promptTokens\": 31,\n",
      "      \"totalTokens\": 39\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 31,\n",
      "    \"output_tokens\": 8,\n",
      "    \"total_tokens\": 39\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const aiMsg = await llm.invoke([\n",
    "    [\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ],\n",
    "    [\"human\", \"I love programming.\"],\n",
    "])\n",
    "aiMsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "console.log(aiMsg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2bfc0-7e78-4528-a73f-499ac150dca8",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "We can [chain](/docs/how_to/sequence/) our model with a prompt template like so:\n",
    "\n",
    "- TODO: Run cells so output can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e197d1d7-a070-4c96-9f8a-a0e86d046e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB4JD9rVBLzTuMee9AabulowEH0d\",\n",
      "  \"content\": \"Ich liebe das Programmieren.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 6,\n",
      "      \"promptTokens\": 26,\n",
      "      \"totalTokens\": 32\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 26,\n",
      "    \"output_tokens\": 6,\n",
      "    \"total_tokens\": 32\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\"\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "    [\n",
    "        [\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ],\n",
    "        [\"human\", \"{input}\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "const chain = prompt.pipe(llm);\n",
    "await chain.invoke(\n",
    "    {\n",
    "        input_language: \"English\",\n",
    "        output_language: \"German\",\n",
    "        input: \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee55bc-ffc8-4cfa-801c-993953a08cfd",
   "metadata": {},
   "source": [
    "## Multimodal messages\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::info\n",
    "This feature is currently in preview. The message schema may change in future releases.\n",
    ":::\n",
    "\n",
    "```\n",
    "\n",
    "OpenAI supports interleaving images with text in input messages with their `gpt-4-vision-preview`. Here's an example of how this looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd55c000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB59AKTPDrSHuTv0y7BNUcM0QDV2\",\n",
      "  \"content\": \"The image shows a classic hot dog, consisting of a grilled or steamed sausage served in the slit of a partially sliced bun. The sausage appears to have grill marks, indicating it may have been cooked on a grill. This is a typical and popular snack or fast food item often enjoyed at sporting events, barbecues, and fairs.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 69,\n",
      "      \"promptTokens\": 438,\n",
      "      \"totalTokens\": 507\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 438,\n",
      "    \"output_tokens\": 69,\n",
      "    \"total_tokens\": 507\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import * as fs from \"node:fs/promises\";\n",
    "\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const imageData2 = await fs.readFile(\"../../../../../examples/hotdog.jpg\");\n",
    "const llm2 = new ChatOpenAI({\n",
    "  model: \"gpt-4-vision-preview\",\n",
    "  maxTokens: 1024,\n",
    "  apiKey: process.env.OPENAI_API_KEY,\n",
    "});\n",
    "const message2 = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"What's in this image?\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url: {\n",
    "        url: `data:image/jpeg;base64,${imageData2.toString(\"base64\")}`,\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "const res2 = await llm2.invoke([message2]);\n",
    "console.log(res2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafbba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB5EWz5AyOHg6UiFkt4HC8H4UZJu\",\n",
      "  \"content\": \"The image contains text that reads \\\"LangChain\\\". Additionally, there is an illustration of a parrot on the left side and two interlinked rings on the right.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 33,\n",
      "      \"promptTokens\": 778,\n",
      "      \"totalTokens\": 811\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 778,\n",
      "    \"output_tokens\": 33,\n",
      "    \"total_tokens\": 811\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const hostedImageMessage3 = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"What does this image say?\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url:\n",
    "        \"https://www.freecodecamp.org/news/content/images/2023/05/Screenshot-2023-05-29-at-5.40.38-PM.png\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "const res3 = await llm2.invoke([hostedImageMessage3]);\n",
    "console.log(res3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3832fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB5IUbzvMo5nsOGYW3jvrQjaCiCg\",\n",
      "  \"content\": \"The image shows a user interface of a digital service or platform called \\\"WebLangChain\\\" which appears to be powered by \\\"Tailify.\\\" There is a prompt that encourages users to \\\"Ask me anything about anything!\\\" Alongside this, there is a text input field labeled \\\"Ask anything...\\\" which also features some example questions or search queries such as \\\"what is langchain?\\\", \\\"history of mesopotamia\\\", \\\"how to build a discord bot\\\", \\\"leonardo dicaprio girlfriend\\\", \\\"fun gift ideas for software engineers\\\", \\\"how does a prism separate light\\\", and \\\"what bear is best\\\". The overall design is clean, with a dark background and a send button represented by a blue icon with a paper airplane, which typically symbolizes sending a message or submitting a query.\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 158,\n",
      "      \"promptTokens\": 101,\n",
      "      \"totalTokens\": 259\n",
      "    },\n",
      "    \"finish_reason\": \"stop\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 101,\n",
      "    \"output_tokens\": 158,\n",
      "    \"total_tokens\": 259\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const lowDetailImage4 = new HumanMessage({\n",
    "  content: [\n",
    "    {\n",
    "      type: \"text\",\n",
    "      text: \"Summarize the contents of this image.\",\n",
    "    },\n",
    "    {\n",
    "      type: \"image_url\",\n",
    "      image_url: {\n",
    "        url: \"https://blog.langchain.dev/content/images/size/w1248/format/webp/2023/10/Screenshot-2023-10-03-at-4.55.29-PM.png\",\n",
    "        detail: \"low\",\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "const res4 = await llm2.invoke([lowDetailImage4]);\n",
    "console.log(res4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39ecb3",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "\n",
    "OpenAI chat models support calling multiple functions to get all required data to answer a question.\n",
    "Here's an example how a conversation turn with this functionality might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65f489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'San Francisco', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_2ytmjITA18j3kLOzzjF5QSC4'\n",
      "  },\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'Tokyo', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_3sU2dCNZ8e8A8wrYlYa7Xq0G'\n",
      "  },\n",
      "  {\n",
      "    name: 'get_current_weather',\n",
      "    args: { location: 'Paris', unit: 'celsius' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_Crmc0QG4x1VHRUyiwPsqzmQS'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// Bind function to the model as a tool\n",
    "const llm5 = new ChatOpenAI({\n",
    "  model: \"gpt-3.5-turbo-1106\",\n",
    "  maxTokens: 128,\n",
    "}).bind({\n",
    "  tools: [\n",
    "    {\n",
    "      type: \"function\",\n",
    "      function: {\n",
    "        name: \"get_current_weather\",\n",
    "        description: \"Get the current weather in a given location\",\n",
    "        parameters: {\n",
    "          type: \"object\",\n",
    "          properties: {\n",
    "            location: {\n",
    "              type: \"string\",\n",
    "              description: \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            unit: { type: \"string\", enum: [\"celsius\", \"fahrenheit\"] },\n",
    "          },\n",
    "          required: [\"location\"],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "  tool_choice: \"auto\",\n",
    "});\n",
    "\n",
    "// Ask initial question that requires multiple tool calls\n",
    "const res5 = await llm5.invoke([\n",
    "  [\"human\", \"What's the weather like in San Francisco, Tokyo, and Paris?\"],\n",
    "]);\n",
    "console.log(res5.tool_calls);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d3a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  \"id\": \"chatcmpl-9rB5Sc3ERHpRymmAAsGS67zczVhAl\",\n",
      "  \"content\": \"The current weather in:\\n- San Francisco is 72°F\\n- Tokyo is 10°C\\n- Paris is 22°C\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 27,\n",
      "      \"promptTokens\": 236,\n",
      "      \"totalTokens\": 263\n",
      "    },\n",
      "    \"finish_reason\": \"stop\",\n",
      "    \"system_fingerprint\": \"fp_adbef9f124\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 236,\n",
      "    \"output_tokens\": 27,\n",
      "    \"total_tokens\": 263\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ToolMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// Mocked out function, could be a database/API call in production\n",
    "function getCurrentWeather(location: string, _unit?: string) {\n",
    "  if (location.toLowerCase().includes(\"tokyo\")) {\n",
    "    return JSON.stringify({ location, temperature: \"10\", unit: \"celsius\" });\n",
    "  } else if (location.toLowerCase().includes(\"san francisco\")) {\n",
    "    return JSON.stringify({\n",
    "      location,\n",
    "      temperature: \"72\",\n",
    "      unit: \"fahrenheit\",\n",
    "    });\n",
    "  } else {\n",
    "    return JSON.stringify({ location, temperature: \"22\", unit: \"celsius\" });\n",
    "  }\n",
    "}\n",
    "\n",
    "// Format the results from calling the tool calls back to OpenAI as ToolMessages\n",
    "const toolMessages5 = res5.additional_kwargs.tool_calls?.map((toolCall) => {\n",
    "  const toolCallResult5 = getCurrentWeather(\n",
    "    JSON.parse(toolCall.function.arguments).location\n",
    "  );\n",
    "  return new ToolMessage({\n",
    "    tool_call_id: toolCall.id,\n",
    "    name: toolCall.function.name,\n",
    "    content: toolCallResult5,\n",
    "  });\n",
    "});\n",
    "\n",
    "// Send the results back as the next step in the conversation\n",
    "const finalResponse5 = await llm5.invoke([\n",
    "  [\"human\", \"What's the weather like in San Francisco, Tokyo, and Paris?\"],\n",
    "  res5,\n",
    "  ...(toolMessages5 ?? []),\n",
    "]);\n",
    "\n",
    "console.log(finalResponse5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067715fe",
   "metadata": {},
   "source": [
    "### `.withStructuredOutput({ ... })`\n",
    "\n",
    "You can also use the `.withStructuredOutput({ ... })` method to coerce `ChatOpenAI` into returning a structured output.\n",
    "\n",
    "The method allows for passing in either a Zod object, or a valid JSON schema (like what is returned from [`zodToJsonSchema`](https://www.npmjs.com/package/zod-to-json-schema)).\n",
    "\n",
    "Using the method is simple. Just define your LLM and call `.withStructuredOutput({ ... })` on it, passing the desired schema.\n",
    "\n",
    "Here is an example using a Zod schema and the `functionCalling` mode (default mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bab2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ operation: 'add', number1: 2, number2: 2 }\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const llm6 = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4-turbo-preview\",\n",
    "});\n",
    "\n",
    "const calculatorSchema6 = z.object({\n",
    "  operation: z.enum([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n",
    "  number1: z.number(),\n",
    "  number2: z.number(),\n",
    "});\n",
    "\n",
    "const modelWithStructuredOutput6 = llm6.withStructuredOutput(calculatorSchema6);\n",
    "\n",
    "const prompt6 = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are VERY bad at math and must always use a calculator.\"],\n",
    "  [\"human\", \"Please help me!! What is 2 + 2?\"],\n",
    "]);\n",
    "const chain6 = prompt6.pipe(modelWithStructuredOutput6);\n",
    "const result6 = await chain6.invoke({});\n",
    "console.log(result6);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e97547",
   "metadata": {},
   "source": [
    "You can also specify `includeRaw` to return the parsed and raw output in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640acaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  raw: AIMessage {\n",
      "    \"id\": \"chatcmpl-9rB5emIYRslBFrUIsC2368dXltljw\",\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_JaH5OB3KYvKF76TUOt6Lp8mu\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": \"[Object]\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 15,\n",
      "        \"promptTokens\": 93,\n",
      "        \"totalTokens\": 108\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"name\": \"calculator\",\n",
      "        \"args\": {\n",
      "          \"number1\": 2,\n",
      "          \"number2\": 2,\n",
      "          \"operation\": \"add\"\n",
      "        },\n",
      "        \"type\": \"tool_call\",\n",
      "        \"id\": \"call_JaH5OB3KYvKF76TUOt6Lp8mu\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 93,\n",
      "      \"output_tokens\": 15,\n",
      "      \"total_tokens\": 108\n",
      "    }\n",
      "  },\n",
      "  parsed: { operation: 'add', number1: 2, number2: 2 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const includeRawModel6 = llm6.withStructuredOutput(calculatorSchema6, {\n",
    "  name: \"calculator\",\n",
    "  includeRaw: true,\n",
    "});\n",
    "\n",
    "const includeRawChain6 = prompt6.pipe(includeRawModel6);\n",
    "const includeRawResult6 = await includeRawChain6.invoke({});\n",
    "console.log(includeRawResult6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f236c",
   "metadata": {},
   "source": [
    "Additionally, you can pass in an OpenAI function definition or JSON schema directly:\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::info\n",
    "If using `jsonMode` as the `method` you must include context in your prompt about the structured output you want. This _must_ include the keyword: `JSON`.\n",
    ":::\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e01d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ number1: 2, number2: 2, operation: 'add' }\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const llm7 = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4-turbo-preview\",\n",
    "});\n",
    "\n",
    "const calculatorSchema7 = {\n",
    "  type: \"object\",\n",
    "  properties: {\n",
    "    operation: {\n",
    "      type: \"string\",\n",
    "      enum: [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "    },\n",
    "    number1: { type: \"number\" },\n",
    "    number2: { type: \"number\" },\n",
    "  },\n",
    "  required: [\"operation\", \"number1\", \"number2\"],\n",
    "};\n",
    "\n",
    "// Default mode is \"functionCalling\"\n",
    "const modelWithStructuredOutput7 = llm7.withStructuredOutput(calculatorSchema7);\n",
    "\n",
    "const prompt7 = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are VERY bad at math and must always use a calculator.\n",
    "Respond with a JSON object containing three keys:\n",
    "'operation': the type of operation to execute, either 'add', 'subtract', 'multiply' or 'divide',\n",
    "'number1': the first number to operate on,\n",
    "'number2': the second number to operate on.\n",
    "`,\n",
    "  ],\n",
    "  [\"human\", \"Please help me!! What is 2 + 2?\"],\n",
    "]);\n",
    "const chain7 = prompt7.pipe(modelWithStructuredOutput7);\n",
    "const result7 = await chain7.invoke({});\n",
    "console.log(result7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae798f49",
   "metadata": {},
   "source": [
    "You can also specify 'includeRaw' to return the parsed and raw output in the result, as well as a \"name\" field to give the LLM additional context as to what you are generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5579e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  raw: AIMessage {\n",
      "    \"id\": \"chatcmpl-9rB5lkylQMLSP9CQ4SaQB9zGw1rP1\",\n",
      "    \"content\": \"{\\n  \\\"operation\\\": \\\"add\\\",\\n  \\\"number1\\\": 2,\\n  \\\"number2\\\": 2\\n}\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"tokenUsage\": {\n",
      "        \"completionTokens\": 25,\n",
      "        \"promptTokens\": 91,\n",
      "        \"totalTokens\": 116\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 91,\n",
      "      \"output_tokens\": 25,\n",
      "      \"total_tokens\": 116\n",
      "    }\n",
      "  },\n",
      "  parsed: { operation: 'add', number1: 2, number2: 2 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const includeRawModel7 = llm7.withStructuredOutput(calculatorSchema7, {\n",
    "  name: \"calculator\",\n",
    "  includeRaw: true,\n",
    "  method: \"jsonMode\",\n",
    "});\n",
    "\n",
    "const includeRawChain7 = prompt7.pipe(includeRawModel7);\n",
    "const includeRawResult7 = await includeRawChain7.invoke({});\n",
    "console.log(includeRawResult7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf343e65",
   "metadata": {},
   "source": [
    "### Disabling parallel tool calls\n",
    "\n",
    "If you have multiple tools bound to the model, but you'd only like for a single tool to be called at a time, you can pass the `parallel_tool_calls` call option to enable/disable this behavior.\n",
    "By default, `parallel_tool_calls` is set to `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb759f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const llm8 = new ChatOpenAI({\n",
    "  temperature: 0,\n",
    "  model: \"gpt-4o\",\n",
    "});\n",
    "\n",
    "// Define your tools\n",
    "const calculatorSchema8 = z\n",
    "  .object({\n",
    "    operation: z.enum([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n",
    "    number1: z.number(),\n",
    "    number2: z.number(),\n",
    "  })\n",
    "  .describe(\"A tool to perform basic arithmetic operations\");\n",
    "const weatherSchema8 = z\n",
    "  .object({\n",
    "    city: z.string(),\n",
    "  })\n",
    "  .describe(\"A tool to get the weather in a city\");\n",
    "\n",
    "// Bind tools to the model\n",
    "const modelWithTools8 = llm8.bindTools([\n",
    "  {\n",
    "    type: \"function\",\n",
    "    function: {\n",
    "      name: \"calculator\",\n",
    "      description: calculatorSchema8.description,\n",
    "      parameters: zodToJsonSchema(calculatorSchema8),\n",
    "    },\n",
    "  },\n",
    "  {\n",
    "    type: \"function\",\n",
    "    function: {\n",
    "      name: \"weather\",\n",
    "      description: weatherSchema8.description,\n",
    "      parameters: zodToJsonSchema(weatherSchema8),\n",
    "    },\n",
    "  },\n",
    "]);\n",
    "\n",
    "// Invoke the model with `parallel_tool_calls` set to `true`\n",
    "const response8 = await modelWithTools8.invoke(\n",
    "  [\"What is the weather in san francisco and what is 23716 times 27342?\"],\n",
    "  {\n",
    "    parallel_tool_calls: true,\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a69645",
   "metadata": {},
   "source": [
    "We can see it called two tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95db614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'weather',\n",
      "    args: { city: 'san francisco' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_FyxazII0M0OgKMnk2UuXDhjv'\n",
      "  },\n",
      "  {\n",
      "    name: 'calculator',\n",
      "    args: { operation: 'multiply', number1: 23716, number2: 27342 },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_raQz2ABUtVpbkruA2K6vBNYd'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "console.log(response8.tool_calls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46a7bb",
   "metadata": {},
   "source": [
    "Invoke the model with `parallel_tool_calls` set to `false`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2bac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "const response9 = await modelWithTools8.invoke(\n",
    "  [\"What is the weather in san francisco and what is 23716 times 27342?\"],\n",
    "  {\n",
    "    parallel_tool_calls: false,\n",
    "  }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d91e9f",
   "metadata": {},
   "source": [
    "We can see it called one tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5731d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: 'weather',\n",
      "    args: { city: 'san francisco' },\n",
      "    type: 'tool_call',\n",
      "    id: 'call_xFbViRUVYj8BFnJIVedU7GVn'\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "console.log(response9.tool_calls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffc86b",
   "metadata": {},
   "source": [
    "## Custom URLs\n",
    "\n",
    "You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a092b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const llm10 = new ChatOpenAI({\n",
    "  temperature: 0.9,\n",
    "  configuration: {\n",
    "    baseURL: \"https://your_custom_url.com\",\n",
    "  },\n",
    "});\n",
    "\n",
    "const message10 = await llm10.invoke(\"Hi there!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b6300",
   "metadata": {},
   "source": [
    "You can also pass other `ClientOptions` parameters accepted by the official SDK.\n",
    "\n",
    "If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/chat/azure).\n",
    "\n",
    "## Calling fine-tuned models\n",
    "\n",
    "You can call fine-tuned OpenAI models by passing in your corresponding `modelName` parameter.\n",
    "\n",
    "This generally takes the form of `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const llm11 = new ChatOpenAI({\n",
    "  temperature: 0.9,\n",
    "  model: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n",
    "});\n",
    "\n",
    "const message11 = await llm11.invoke(\"Hi there!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2270901",
   "metadata": {},
   "source": [
    "## Generation metadata\n",
    "\n",
    "If you need additional information like logprobs or token usage, these will be returned directly in the `.invoke` response.\n",
    "\n",
    "```{=mdx}\n",
    "\n",
    ":::tip\n",
    "Requires `@langchain/core` version >=0.1.48.\n",
    ":::\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b675330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.0004585519,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.000049305523,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.000029517714,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.00073185476,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -9.0883464e-7,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.104538105,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -6.704273e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000052643223,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.00001247159,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// See https://cookbook.openai.com/examples/using_logprobs for details\n",
    "const llm12 = new ChatOpenAI({\n",
    "  logprobs: true,\n",
    "  // topLogprobs: 5,\n",
    "});\n",
    "\n",
    "const responseMessage12 = await llm12.invoke(\"Hi there!\");\n",
    "console.dir(responseMessage12.response_metadata.logprobs, { depth: null });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c364f",
   "metadata": {},
   "source": [
    "### With callbacks\n",
    "\n",
    "You can also use the callbacks system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e74121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.0005182436,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.000040246043,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.000035716304,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.0006764544,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -0.0000010280384,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.12827769,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -4.3202e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000059914648,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.000012352386,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "// See https://cookbook.openai.com/examples/using_logprobs for details\n",
    "const llm13 = new ChatOpenAI({\n",
    "  logprobs: true,\n",
    "  // topLogprobs: 5,\n",
    "});\n",
    "\n",
    "const result13 = await llm13.invoke(\"Hi there!\", {\n",
    "  callbacks: [\n",
    "    {\n",
    "      handleLLMEnd(output) {\n",
    "        console.dir(output.generations[0][0].generationInfo.logprobs, { depth: null });\n",
    "      },\n",
    "    },\n",
    "  ],\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9f01aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  content: [\n",
      "    {\n",
      "      token: 'Hello',\n",
      "      logprob: -0.0005182436,\n",
      "      bytes: [ 72, 101, 108, 108, 111 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '!',\n",
      "      logprob: -0.000040246043,\n",
      "      bytes: [ 33 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' How',\n",
      "      logprob: -0.000035716304,\n",
      "      bytes: [ 32, 72, 111, 119 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' can',\n",
      "      logprob: -0.0006764544,\n",
      "      bytes: [ 32, 99, 97, 110 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' I',\n",
      "      logprob: -0.0000010280384,\n",
      "      bytes: [ 32, 73 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' assist',\n",
      "      logprob: -0.12827769,\n",
      "      bytes: [\n",
      "         32,  97, 115,\n",
      "        115, 105, 115,\n",
      "        116\n",
      "      ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' you',\n",
      "      logprob: -4.3202e-7,\n",
      "      bytes: [ 32, 121, 111, 117 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: ' today',\n",
      "      logprob: -0.000059914648,\n",
      "      bytes: [ 32, 116, 111, 100, 97, 121 ],\n",
      "      top_logprobs: []\n",
      "    },\n",
      "    {\n",
      "      token: '?',\n",
      "      logprob: -0.000012352386,\n",
      "      bytes: [ 63 ],\n",
      "      top_logprobs: []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "console.dir(result13.response_metadata.logprobs, { depth: null });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194627d",
   "metadata": {},
   "source": [
    "## Streaming tokens\n",
    "\n",
    "OpenAI supports streaming token counts via an opt-in call option. This can be set by passing `{ stream_options: { include_usage: true } }`.\n",
    "Setting this call option will cause the model to return an additional chunk at the end of the stream, containing the token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6efaebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ input_tokens: 13, output_tokens: 33, total_tokens: 46 }\n"
     ]
    }
   ],
   "source": [
    "import type { AIMessageChunk } from \"@langchain/core/messages\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { concat } from \"@langchain/core/utils/stream\";\n",
    "\n",
    "// Instantiate the model\n",
    "const llm14 = new ChatOpenAI();\n",
    "\n",
    "const response14 = await llm14.stream(\"Hello, how are you?\", {\n",
    "  // Pass the stream options\n",
    "  stream_options: {\n",
    "    include_usage: true,\n",
    "  },\n",
    "});\n",
    "\n",
    "// Iterate over the response, only saving the last chunk\n",
    "let finalResult14: AIMessageChunk | undefined;\n",
    "for await (const chunk14 of response14) {\n",
    "  finalResult14 = !finalResult14 ? chunk14 : concat(finalResult14, chunk14);\n",
    "}\n",
    "\n",
    "console.log(finalResult14?.usage_metadata);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb5ca-c3ae-4a58-be67-2cd18574b9a3",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For detailed documentation of all ChatOpenAI features and configurations head to the API reference: https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
