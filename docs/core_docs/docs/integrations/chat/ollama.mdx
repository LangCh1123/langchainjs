---
sidebar_label: Ollama
---

# ChatOllama

[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 2, locally.

Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.

This example goes over how to use LangChain to interact with an Ollama-run Llama 2 7b instance as a chat model.
For a complete list of supported models and model variants, see the [Ollama model library](https://github.com/jmorganca/ollama#model-library).

## Setup

Follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance. Then, download the `@langchain/ollama` package.

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/ollama
```

## Usage

import CodeBlock from "@theme/CodeBlock";
import OllamaExample from "@examples/models/chat/integration_ollama.ts";

<CodeBlock language="typescript">{OllamaExample}</CodeBlock>

## Tools

Ollama now offers support for native tool calling. The example below demonstrates how you can invoke a tool from an Ollama model.

import OllamaToolsExample from "@examples/models/chat/integration_ollama_tools.ts";

<CodeBlock language="typescript">{OllamaToolsExample}</CodeBlock>

:::tip
You can see the LangSmith trace of the above example [here](https://smith.langchain.com/public/e98d812b-82cd-43a3-9760-e8a4f418c273/r)
:::

Since `ChatOllama` supports the `.bindTools()` method, you can also call `.withStructuredOutput()` to get a structured output from the tool.

import OllamaWSOExample from "@examples/models/chat/integration_ollama_wso.ts";

<CodeBlock language="typescript">{OllamaWSOExample}</CodeBlock>

:::tip
You can see the LangSmith trace of the above example [here](https://smith.langchain.com/public/ed113c53-1299-4814-817e-1157c9eac47e/r)
:::

## JSON mode

Ollama also supports a JSON mode that coerces model outputs to only return JSON. Here's an example of how this can be useful for extraction:

import OllamaJSONModeExample from "@examples/models/chat/integration_ollama_json_mode.ts";

<CodeBlock language="typescript">{OllamaJSONModeExample}</CodeBlock>

:::tip
You can see a simple LangSmith trace of this [here](https://smith.langchain.com/public/1fbd5660-b7fd-41c3-9d3a-a6ecc735277c/r)
:::

## Multimodal models

Ollama supports open source multimodal models like [LLaVA](https://ollama.ai/library/llava) in versions 0.1.15 and up.
You can pass images as part of a message's `content` field to multimodal-capable models like this:

import OllamaMultimodalExample from "@examples/models/chat/integration_ollama_multimodal.ts";

<CodeBlock language="typescript">{OllamaMultimodalExample}</CodeBlock>

This will currently not use the image's position within the prompt message as additional information, and will just pass
the image along as context with the rest of the prompt messages.
