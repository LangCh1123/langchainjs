import { BasePromptTemplate } from "@langchain/core/prompts";
import { LanguageModelLike } from "@langchain/core/language_models/base";
import { Document } from "@langchain/core/documents";
import {
  Runnable,
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";

import {
  DEFAULT_DOCUMENT_PROMPT,
  DOCUMENTS_KEY,
  INTERMEDIATE_STEPS_KEY,
} from "./base.js";

const OUTPUT_KEY = "output";

interface IntermediateInputs {
  [INTERMEDIATE_STEPS_KEY]: string[];
  [OUTPUT_KEY]: string;
}

/**
 * Create a chain that feeds documents to a model one at a time and updates the output.
 * 
 * @param llm Language model to use for responding.
 * @param initialPrompt The prompt to use on the first document. Must accept "context" 
    as one of the input variables. The first document will be passed in as "context".
 * @param refinePrompt The prompt to use on all subsequent documents. Must accept 
    "context" and "output" as input variables. A document will be passed in as 
    "context" and the refined output up to this iteration will be passed in as
    "output.
 * @param documentPrompt Prompt used for formatting each document into a string. Input
    variables can be "page_content" or any metadata keys that are in all documents.
    "page_content" will automatically retrieve the `Document.page_content`, and all 
    other inputs variables will be automatically retrieved from the `Document.metadata` dictionary. Default to a prompt that only contains `Document.page_content`.
 * @returns An LCEL `Runnable` chain. 
    Expects a dictionary as input with a list of `Document`s being passed under 
    the "context" key. 
    Returns a dictionary as output. The output dictionary contains two keys, 
    "output" and "intermediate_steps". "output" contains the final output. 
    "intermediate_steps" contains the list of intermediate output 
    strings generated by the chain, in the order that they were generated.
 *
 * @example
 * ```ts
 * import { ChatOpenAI } from "@langchain/openai";
 * import { ChatPromptTemplate } from "@langchain_core/prompts/chat";
 * 
 * const llm = new ChatOpenAI();
 * const initialPrompt = ChatPromptTemplate.fromMessages([
 *  ["system", "Summarize this information:"],
 *  ["user", {context}"]
 * ])
 * const refinePrompt = ChatPromptTemplate.fromMessages([
 *  ["system", `You are summarizing a long document one page at a time.
      You have summarized part of the document. Given the next page, update your
      summary. Respond with only the updated summary and no other text.
      Here is your working summary:\n\n{output}.`],
 *  ["user", "Here is the next page:\n\n{context}"]
 * ])
 * 
 * const chain = await createRefineDocumentsChain(llm, initialPrompt, refinePrompt);
 * const output = await chain.invoke({ context: documents, question: "..." });
 */
export async function createRefineDocumentsChain(
  llm: LanguageModelLike,
  initialPrompt: BasePromptTemplate,
  refinePrompt: BasePromptTemplate,
  documentPrompt: BasePromptTemplate = DEFAULT_DOCUMENT_PROMPT
) {
  if (!initialPrompt.inputVariables.includes(DOCUMENTS_KEY)) {
    throw new Error(`Initial prompt must include ${DOCUMENTS_KEY} variable`);
  }
  if (!refinePrompt.inputVariables.includes(DOCUMENTS_KEY)) {
    throw new Error(`Refine prompt must include ${DOCUMENTS_KEY} variable`);
  }
  if (!refinePrompt.inputVariables.includes(OUTPUT_KEY)) {
    throw new Error(`Refine prompt must include ${OUTPUT_KEY} variable`);
  }
  // work on first document in the list, creating first intermediate output
  const initialStep = RunnablePassthrough.assign({
    [OUTPUT_KEY]: RunnableSequence.from(
      [
        formatNextDoc.bind(null, documentPrompt),
        initialPrompt,
        llm,
        new StringOutputParser(),
      ],
      "initial_step"
    ),
  });
  // work on next document in the list, creating next intermediate output
  const refineStep = RunnablePassthrough.assign({
    [OUTPUT_KEY]: RunnableSequence.from(
      [
        formatNextDoc.bind(null, documentPrompt),
        refinePrompt,
        llm,
        new StringOutputParser(),
      ],
      "refine_step"
    ),
    [INTERMEDIATE_STEPS_KEY]: (inputs: IntermediateInputs) => [
      ...inputs[INTERMEDIATE_STEPS_KEY],
      inputs[OUTPUT_KEY],
    ],
  });
  return RunnableSequence.from(
    [
      // first document
      initialStep,
      // remaining documents
      (inputs: Record<string, Document[]>) => {
        const docs = inputs[DOCUMENTS_KEY];
        if (docs.length <= 1) {
          return new RunnablePassthrough();
        } else if (docs.length === 2) {
          return refineStep;
        } else {
          return RunnableSequence.from(
            docs.slice(1).map(() => refineStep) as unknown as [
              Runnable,
              ...Runnable[],
              Runnable
            ],
            "refine_documents_chain"
          );
        }
      },
    ],
    "refine_documents_chain"
  ).pick([OUTPUT_KEY, INTERMEDIATE_STEPS_KEY]);
}

function formatNextDoc(
  documentPrompt: BasePromptTemplate,
  inputs: Record<string, Document[]>
) {
  const nextIdx = (inputs[INTERMEDIATE_STEPS_KEY] ?? []).length;
  const nextDoc = inputs[DOCUMENTS_KEY][nextIdx];
  return documentPrompt.invoke({ page_content: nextDoc.pageContent });
}
